---
title: "Decision Tree Challenge"
subtitle: "Feature Importance and Categorical Variable Encoding"
format:
  html: default
  pdf: default
execute:
  echo: true
  eval: true
---

# üå≥ Decision Tree Challenge - Feature Importance and Variable Encoding

## Challenge Overview

**Your Mission:** Create a comprehensive Quarto document that demonstrates how decision trees measure feature importance, analyzes the critical differences between categorical and numerical variable encoding, and presents compelling evidence of why proper data preprocessing matters for interpretable machine learning. Then render the document to HTML and deploy it via GitHub Pages using the starter repository workflow.

::: {.callout-warning}
## ‚ö†Ô∏è AI Partnership Required

This challenge pushes boundaries intentionally. You'll tackle problems that normally require weeks of study, but with Cursor AI as your partner (and your brain keeping it honest), you can accomplish more than you thought possible.

**The new reality:** The four stages of competence are Ignorance ‚Üí Awareness ‚Üí Learning ‚Üí Mastery. AI lets us produce Mastery-level work while operating primarily in the Awareness stage. I focus on awareness training, you leverage AI for execution, and together we create outputs that used to require years of dedicated study.
:::

## The Decision Tree Problem üéØ

> "The most important thing in communication is hearing what isn't said." - Peter Drucker

**The Core Problem:** Decision trees are often praised for their interpretability and ability to handle both numerical and categorical variables. But what happens when we encode categorical variables as numbers? How does this affect our understanding of feature importance?

**What is Feature Importance?** In decision trees, feature importance measures how much each variable contributes to reducing impurity (or improving prediction accuracy) across all splits in the tree. It's a key metric for understanding which variables matter most for your predictions.

::: {.callout-important}
## üéØ The Key Insight: Encoding Matters for Interpretability

**The problem:** When we encode categorical variables as numerical values (like 1, 2, 3, 4...), decision trees treat them as if they have a meaningful numerical order. This can completely distort our understanding of feature importance.

**Why this matters:** If a categorical variable like "Neighborhood Quality" (Poor, Fair, Good, Excellent) is encoded as (1, 2, 3, 4), the tree might split on "Neighborhood Quality > 2.5" instead of recognizing that this is really a categorical choice between discrete categories.

**The connection:** Proper encoding preserves the true nature of categorical variables and gives us accurate feature importance rankings.
:::

**The Real-World Context:** In real estate, we know that neighborhood quality, house style, and other categorical factors are crucial for predicting home prices. But if we encode these as numbers, we might get misleading insights about which features actually matter most.

**The Devastating Reality:** Even sophisticated machine learning models can give us completely wrong insights about feature importance if we don't properly encode our variables. A categorical variable that should be among the most important might appear irrelevant, while a numerical variable might appear artificially important.

Your challenge is to explore the Ames Housing dataset and show how this happens:

$$
\begin{aligned}
\text{Price} &\equiv \text{House Sale Price (our target variable)}\\
\text{Neighborhood} &\equiv \text{Categorical: Ames neighborhood}\\
\text{House Style} &\equiv \text{Categorical: Style of dwelling}\\
\text{Year Built} &\equiv \text{Numerical: Year house was constructed}\\
\text{Square Feet} &\equiv \text{Numerical: Total square footage}
\end{aligned}
$$

Let's assume we want to predict house prices and understand which features matter most. The key question is: **How does encoding categorical variables as numbers affect our understanding of feature importance?**

## The Ames Housing Dataset üè†

The Ames Housing dataset contains detailed information about residential properties sold in Ames, Iowa from 2006 to 2010. This dataset is perfect for our analysis because it contains both categorical variables (like neighborhood, house style, quality ratings) and numerical variables (like square footage, year built, number of bedrooms).

### Data Loading and Initial Exploration

::: {.panel-tabset}

### R

```{r}
#| label: load-data-r
#| echo: true
#| message: false
#| warning: false

# Load required libraries
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(rpart))
suppressPackageStartupMessages(library(rpart.plot))

# Load the Ames Housing dataset
# Note: This loads the data from the causact package
# First download the file, then load it
temp_file <- tempfile(fileext = ".rda")
download.file("https://github.com/flyaflya/causact/raw/refs/heads/master/data/houseDF.rda", 
              temp_file, mode = "wb")
load(temp_file)
unlink(temp_file)  # Clean up temporary file

# Display basic information about the dataset
cat("Dataset dimensions:", dim(houseDF), "\n")
cat("Number of variables:", ncol(houseDF), "\n")
cat("Number of observations:", nrow(houseDF), "\n\n")

# Show first few rows
head(houseDF, 10)
```

```{r}
#| label: data-structure-r
#| echo: true
#| message: false
#| warning: false

# Examine data types and structure
str(houseDF)

# Summary statistics for key variables
summary(houseDF[c("SalePrice", "Neighborhood", "HouseStyle", "YearBuilt", "GrLivArea")])
```

### Python

```{python}
#| label: load-data-python
#| echo: true

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.tree import DecisionTreeRegressor, plot_tree
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import warnings
warnings.filterwarnings('ignore')

# Load the Ames Housing dataset
# Note: This loads the data from the causact package
url = "https://github.com/flyaflya/causact/raw/refs/heads/master/data/houseDF.rda"

# For Python, we'll need to use pyreadr to read R data files
import pyreadr
import requests
import tempfile
import os

# Download the file first, then read it
response = requests.get(url)
temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.rda')
temp_file.write(response.content)
temp_file.close()

# Read the R data file
result = pyreadr.read_r(temp_file.name)
houseDF = result['houseDF']

# Clean up temporary file
os.unlink(temp_file.name)

print("Dataset loaded successfully!")


# Display basic information about the dataset
print(f"Dataset dimensions: {houseDF.shape}")
print(f"Number of variables: {houseDF.shape[1]}")
print(f"Number of observations: {houseDF.shape[0]}\n")

# Show first few rows
print("First 10 rows:")
houseDF.head(10)
```

```{python}
#| label: data-structure-python
#| echo: true

# Examine data types and structure
print("Data types:")
print(houseDF.dtypes)
print("\nData structure:")
print(houseDF.info())

# Summary statistics for key variables
print("\nSummary statistics for key variables:")
houseDF[['SalePrice', 'Neighborhood', 'HouseStyle', 'YearBuilt', 'GrLivArea']].describe(include='all')
```

### Adding Zip Code Information

Now let's enhance our dataset by adding zip code information for each neighborhood. This will make our analysis more realistic and provide another categorical variable to work with:

::: {.panel-tabset}

### R

```{r}
#| label: add-zip-codes-r
#| echo: true
#| message: false
#| warning: false

# Add zip code information for each neighborhood
# Ames, Iowa zip codes for different neighborhoods
neighborhood_zip_mapping <- c(
  "Blmngtn" = "50010",  # Bloomington Heights
  "Blueste" = "50010",  # Bluestem
  "BrDale" = "50010",   # Briardale
  "BrkSide" = "50010",  # Brookside
  "ClearCr" = "50010",  # Clear Creek
  "CollgCr" = "50010",  # College Creek
  "Crawfor" = "50010",  # Crawford
  "Edwards" = "50010",  # Edwards
  "Gilbert" = "50010",  # Gilbert
  "IDOTRR" = "50010",   # Iowa DOT and Rail Road
  "MeadowV" = "50010",  # Meadow Village
  "Mitchel" = "50010",  # Mitchell
  "Names" = "50010",    # North Ames
  "NoRidge" = "50014",  # Northridge
  "NPkVill" = "50010",  # Northpark Villa
  "NridgHt" = "50010",  # Northridge Heights
  "NWAmes" = "50010",   # Northwest Ames
  "OldTown" = "50010",  # Old Town
  "SWISU" = "50010",    # South & West of Iowa State University
  "Sawyer" = "50010",   # Sawyer
  "SawyerW" = "50010",  # Sawyer West
  "Somerst" = "50010",  # Somerset
  "StoneBr" = "50010",  # Stone Brook
  "Timber" = "50010",   # Timberland
  "Veenker" = "50010",  # Veenker
  "NAmes" = "50010"     # North Ames (alternative spelling)
)

# Add zip code column to the dataset
houseDF$ZipCode <- neighborhood_zip_mapping[houseDF$Neighborhood]

# Display unique neighborhoods and their zip codes
neighborhood_zip_df <- data.frame(
  Neighborhood = names(neighborhood_zip_mapping),
  ZipCode = neighborhood_zip_mapping,
  stringsAsFactors = FALSE
)

print("Neighborhood to Zip Code Mapping:")
print(neighborhood_zip_df)

# Show distribution of properties by zip code
zip_distribution <- table(houseDF$ZipCode)
print("\nDistribution of properties by Zip Code:")
print(zip_distribution)
```

### Python

```{python}
#| label: add-zip-codes-python
#| echo: true

# Add zip code information for each neighborhood
# Ames, Iowa zip codes for different neighborhoods
neighborhood_zip_mapping = {
    "Blmngtn": "50010",  # Bloomington Heights
    "Blueste": "50010",  # Bluestem
    "BrDale": "50010",   # Briardale
    "BrkSide": "50010",  # Brookside
    "ClearCr": "50010",  # Clear Creek
    "CollgCr": "50010",  # College Creek
    "Crawfor": "50010",  # Crawford
    "Edwards": "50010",  # Edwards
    "Gilbert": "50010",  # Gilbert
    "IDOTRR": "50010",   # Iowa DOT and Rail Road
    "MeadowV": "50010",  # Meadow Village
    "Mitchel": "50010",  # Mitchell
    "Names": "50010",    # North Ames
    "NoRidge": "50010",  # Northridge
    "NPkVill": "50010",  # Northpark Villa
    "NridgHt": "50010",  # Northridge Heights
    "NWAmes": "50010",   # Northwest Ames
    "OldTown": "50010",  # Old Town
    "SWISU": "50010",    # South & West of Iowa State University
    "Sawyer": "50010",   # Sawyer
    "SawyerW": "50010",  # Sawyer West
    "Somerst": "50010",  # Somerset
    "StoneBr": "50010",  # Stone Brook
    "Timber": "50010",   # Timberland
    "Veenker": "50010",  # Veenker
    "NAmes": "50010"     # North Ames (alternative spelling)
}

# Add zip code column to the dataset
houseDF['ZipCode'] = houseDF['Neighborhood'].map(neighborhood_zip_mapping)

# Display unique neighborhoods and their zip codes
neighborhood_zip_df = pd.DataFrame(list(neighborhood_zip_mapping.items()), 
                                  columns=['Neighborhood', 'ZipCode'])

print("Neighborhood to Zip Code Mapping:")
print(neighborhood_zip_df)

# Show distribution of properties by zip code
zip_distribution = houseDF['ZipCode'].value_counts()
print("\nDistribution of properties by Zip Code:")
print(zip_distribution)
```

:::

::: {.callout-note}
## üìù Why Add Zip Codes?

**Enhanced Analysis:** Adding zip code information provides several benefits for our decision tree challenge:

1. **Additional Categorical Variable:** Zip codes are perfect categorical variables with no inherent numerical order
2. **Real-World Context:** Makes the analysis more realistic and relatable
3. **Encoding Challenge:** Demonstrates how different categorical variables (neighborhood vs. zip code) might be encoded differently
4. **Feature Importance:** Shows how multiple categorical variables interact in feature importance rankings

**The Ames Context:** All neighborhoods in Ames, Iowa fall under the 50010 zip code, which is realistic for a small city. In larger cities, we might see more zip code variation, but this still demonstrates the concept effectively.
:::

:::

### Dataset Description

The Ames Housing dataset contains **1,460 observations** of residential properties with **37 variables** describing various aspects of each property. Key variables include:

**Target Variable:**
- `SalePrice`: The sale price of the house (our prediction target)

**Categorical Variables (Perfect for Our Analysis):**
- `Neighborhood`: Ames neighborhood (e.g., OldTown, Edwards, BrkSide, Sawyer, NAmes)
- `ZipCode`: Zip code for each neighborhood (50010 for all Ames neighborhoods)
- `HouseStyle`: Style of dwelling (e.g., 1Story, 2Story, 1.5Fin, SLvl, SFoyer)
- `OverallQual`: Overall material and finish quality (1-10 scale)
- `OverallCond`: Overall condition rating (1-10 scale)

**Numerical Variables:**
- `YearBuilt`: Year the house was originally built
- `GrLivArea`: Above ground living area square feet
- `TotalBsmtSF`: Total square feet of basement area
- `GarageCars`: Size of garage in car capacity

::: {.callout-note}
## üìù Why This Dataset is Perfect for Our Analysis

**The Challenge:** This dataset is ideal for demonstrating the categorical vs. numerical encoding problem because:

1. **Clear categorical variables:** Neighborhood and HouseStyle have no inherent numerical order
2. **Mixed data types:** We have both truly categorical and truly numerical variables
3. **Real-world relevance:** House prices are influenced by both categorical factors (neighborhood quality) and numerical factors (square footage)
4. **Interpretable results:** We can easily understand which features should matter most for house prices

**The Key Question:** When we build decision trees to predict house prices, how does encoding categorical variables as numbers affect our understanding of feature importance?
:::

## Challenge Requirements üìã

### Minimum Requirements for Any Points on Challenge

1. **Create a Quarto Document:** Use the starter repository (see Repository Setup section below) to begin with a working template. Write a concise quarto markdown file that includes a narrative of what you are doing along with the requested code, results, and visualizations of your decision tree analyses.

2. **Render to HTML:** You must render the quarto markdown file to HTML.

3. **GitHub Repository:** Use your forked repository (from the starter repository) named "decisionTreeChallenge" in your GitHub account. Upload your rendered HTML files to this repository.

4. **GitHub Pages Setup:** The repository should be made the source of your github pages:

   - Go to your repository settings (click the "Settings" tab in your GitHub repository)
   - Scroll down to the "Pages" section in the left sidebar
   - Under "Source", select "Deploy from a branch"
   - Choose "main" branch and "/ (root)" folder
   - Click "Save"
   - Your site will be available at: `https://[your-username].github.io/decisionTreeChallenge/`
   - **Note:** It may take a few minutes for the site to become available after enabling Pages

## Getting Started: Repository Setup üöÄ

::: {.callout-important}
## üìÅ Quick Start with Starter Repository

**Step 1:** Fork the starter repository to your github account at [https://github.com/flyaflya/decisionTreeChallenge.git](https://github.com/flyaflya/decisionTreeChallenge.git)

**Step 2:** Clone your fork locally using Cursor (or VS Code)

**Step 3:** You're ready to start! The repository includes pre-loaded data and a working template.
:::

::: {.callout-tip}
## üí° Why Use the Starter Repository?

**Benefits:**

- **Pre-loaded data:** All required data (Ames Housing dataset) is included
- **Working template:** Basic Quarto structure (`index.qmd`) is ready
- **No setup errors:** Avoid common data loading issues
- **Focus on analysis:** Spend time on decision tree analysis, not data preparation
:::

### Getting Started Tips

::: {.callout-note}
## üéØ Navy SEALs Motto

> "Slow is Smooth and Smooth is Fast"

*Take your time to understand the decision tree mechanics, plan your approach carefully, and execute with precision. Rushing through this challenge will only lead to errors and confusion.*
:::

::: {.callout-warning}
## üíæ Important: Save Your Work Frequently!

**Before you start coding:** Make sure to commit your work often using the Source Control panel in Cursor (Ctrl+Shift+G or Cmd+Shift+G). This prevents the AI from overwriting your progress and ensures you don't lose your work.

**Commit after each major step:**

- After completing each decision tree analysis
- After finishing each challenge question
- Before asking the AI for help with new code

**How to commit:**

1. Open Source Control panel (Ctrl+Shift+G)
2. Stage your changes (+ button)
3. Write a descriptive commit message
4. Click the checkmark to commit

*Remember: Frequent commits are your safety net!*
:::

## Grading Rubric üéì

::: {.callout-important}
## üìä What You're Really Being Graded On

**This is an investigative report, not a coding exercise.** You're analyzing decision tree models and reporting your findings like a professional data scientist would. Think of this as a brief you'd write for a client or manager about why proper data encoding matters for machine learning interpretability.

**What makes a great report:**

- **Clear narrative:** Tell the story of what you discovered about feature importance and encoding
- **Hidden Code:** Tell a narrative and visual story, but hide your code (the code can be referenced in your github *.qmd source file if needed).
- **Insightful analysis:** Focus on the most interesting differences between proper and improper encoding
- **Professional presentation:** Clean, readable, and engaging
- **Concise conclusions:** No AI babble or unnecessary technical jargon
- **Human insights:** Your interpretation of what the feature importance rankings actually mean (or don't mean)
- **Use convention of dependent variable on the vertical axis:** When plotting, put the dependent variable (i.e., SalePrice) on the vertical axis. Independent variables should be on the horizontal axis.

**What we're looking for:** A compelling 4-8 minute read that demonstrates both the power of decision trees for interpretability and the critical importance of proper data preprocessing for accurate feature importance analysis.
:::

### Questions to Answer for 75% Grade on Challenge

1. **Decision Tree with Proper Encoding:** Build a decision tree to predict house prices using properly encoded categorical variables (as factors/categories, not numbers). What are the feature importance rankings? Which variables appear most important?

2. **Visualization of Decision Tree:** Create a visualization of your decision tree showing the splits and decision paths. Comment on the interpretability and any interesting patterns.

3. **Decision Tree with Improper Encoding:** Build a decision tree using the same variables, but encode categorical variables as numerical values (1, 2, 3, 4...). What are the feature importance rankings? How do they compare to the properly encoded model?

::: {.callout-tip}
## üéØ Remember the Key Insight!

When analyzing your decision tree results, focus on:

**Feature Importance Comparison:**

- Are the rankings different between proper and improper encoding?
- Do categorical variables appear more or less important when encoded as numbers?
- Which encoding gives more interpretable and realistic results?

**Key Questions:**

- If Neighborhood is encoded as (1, 2, 3, 4...), does the tree treat it as if there's a meaningful order?
- Do the feature importance rankings make sense from a real estate perspective?
- Which model would you trust more for understanding what drives house prices?
:::

### Questions to Answer for 85% Grade on Challenge

4. **Model Performance Comparison:** Compare the R-squared values and prediction accuracy between the two decision tree models. Do both models perform similarly, or does encoding affect predictive performance?

5. **Feature Importance Analysis:** Create side-by-side visualizations comparing feature importance between the two models. Which variables show the biggest differences in importance rankings?

### Questions to Answer for 95% Grade on Challenge

6. **Real-World Implications:** Assume both models were used by a real estate company to understand what drives house prices. What recommendations would each model lead to? Which model would give better business insights?

### Questions to Answer for 100% Grade on Challenge

7. **Advanced Analysis:** Investigate how different encoding methods (one-hot encoding, label encoding, ordinal encoding) affect feature importance. Which encoding method gives the most interpretable and accurate feature importance rankings?

::: {.callout-tip}
## üéØ For 100% Grade: Focus on What's Most Interesting

**The key insight:** Proper data preprocessing is crucial for interpretable machine learning. The same model can give completely different insights about feature importance depending on how we encode our variables.

**What to investigate:**

- **Feature Importance Interpretation:** What do the rankings actually mean in each case?
- **The "Encoding Trap":** How can improper encoding lead to misleading business insights?
- **Best Practices:** What encoding methods should data scientists use for categorical variables?

**Write like a data science consultant:** Your report should help someone understand not just what the numbers show, but why proper encoding matters and what to do about it.
:::

## Technical Implementation Preferences üí°

### Setting Up Your Analysis

**For R Users:**

- Use `tidyverse` for data manipulation
- Use `rpart` for decision tree modeling
- Use `rpart.plot` for tree visualization
- Use `ggplot2` for feature importance plots

**For Python Users:**

- Use `pandas` for data manipulation
- Use `sklearn.tree.DecisionTreeRegressor` for decision tree modeling
- Use `matplotlib` and `seaborn` for visualizations
- Use `plot_tree` for tree visualization

### Visualization Preferences

- **Professional Styling:** Use consistent colors, clear labels, readable fonts, and informative titles
- **Feature Importance Plots:** Use horizontal bar charts for easy comparison
- **Tree Visualizations:** Keep trees readable and focused on key splits

## Submission Checklist ‚úÖ

**Minimum Requirements (Required for Any Points):**

- [ ] Forked starter repository from [https://github.com/flyaflya/decisionTreeChallenge.git](https://github.com/flyaflya/decisionTreeChallenge.git)
- [ ] Cloned repository locally using Cursor (or VS Code)
- [ ] Quarto document updated with clear narrative (use the provided `index.qmd` template)
- [ ] Document rendered to HTML successfully
- [ ] HTML files uploaded to your forked repository
- [ ] GitHub Pages enabled and working
- [ ] Site accessible at `https://[your-username].github.io/decisionTreeChallenge/`

**75% Grade Requirements:**

- [ ] Decision tree with proper encoding and feature importance analysis (Question 1)
- [ ] Decision tree visualization with interpretation (Question 2)
- [ ] Decision tree with improper encoding and comparison (Question 3)

**85% Grade Requirements:**

- [ ] Model performance comparison between encoding methods (Question 4)
- [ ] Side-by-side feature importance visualizations (Question 5)

**95% Grade Requirements:**

- [ ] Real-world implications and business insights analysis (Question 6)

**100% Grade Requirements:**

- [ ] Advanced encoding methods comparison (Question 7)

**Code Quality (All Grades):**

- [ ] Clean, well-commented code
- [ ] Appropriate use of decision tree functions
- [ ] Professional visualization styling
- [ ] Reproducible results

**Report Quality (Critical for Higher Grades):**

- [ ] Clear, engaging narrative that tells a story
- [ ] Focus on the most interesting findings about feature importance and encoding
- [ ] Professional writing style (no AI-generated fluff)
- [ ] Concise analysis that gets to the point
- [ ] Practical insights that would help a real data scientist
- [ ] Visualizations that support your narrative, not overwhelm it

### Resources

- **Quarto Markdown:** [quarto.org/docs/authoring/markdown-basics.html](https://quarto.org/docs/authoring/markdown-basics.html)
- **Quarto Documentation:** [quarto.org/docs](https://quarto.org/docs)
- **R for Data Science:** [r4ds.had.co.nz](https://r4ds.had.co.nz)
- **Python Data Science Handbook:** [jakevdp.github.io/PythonDataScienceHandbook](https://jakevdp.github.io/PythonDataScienceHandbook)
- **Decision Trees:** [An Introduction to Statistical Learning](https://www.statlearning.com/)

## Essential Decision Tree Concepts üéØ {#sec-decision-tree-concepts}

Before diving into the challenge, let's review the key decision tree concepts you'll need. These examples will prepare you for the feature importance and encoding analysis.

### 1. Simple Decision Tree: The Basics

Let's start with a basic decision tree to understand the mechanics:

::: {.panel-tabset}

### R

```{r}
#| label: simple-tree-r
#| fig-cap: R simple decision tree example
#| echo: true

suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(rpart))
suppressPackageStartupMessages(library(rpart.plot))

# Create simple example data
set.seed(123)
simple_data <- tibble(
  square_feet = rnorm(100, mean = 1500, sd = 300),
  bedrooms = sample(1:4, 100, replace = TRUE),
  price = 100000 + 50 * square_feet + 10000 * bedrooms + rnorm(100, 0, 10000)
)

# Fit decision tree
tree_model <- rpart(price ~ square_feet + bedrooms, data = simple_data)

# Display results
print(tree_model)

# Plot the tree
rpart.plot(tree_model, main = "Simple Decision Tree Example")
```

### Python

```{python}
#| label: simple-tree-python
#| fig-cap: Python simple decision tree example
#| echo: true

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor, plot_tree
from sklearn.metrics import r2_score

# Set seed for reproducibility
np.random.seed(123)

# Create simple example data
n = 100
square_feet = np.random.normal(1500, 300, n)
bedrooms = np.random.choice([1, 2, 3, 4], n)
price = 100000 + 50 * square_feet + 10000 * bedrooms + np.random.normal(0, 10000, n)

# Create DataFrame
simple_data = pd.DataFrame({
    'square_feet': square_feet,
    'bedrooms': bedrooms,
    'price': price
})

# Fit decision tree
tree_model = DecisionTreeRegressor(max_depth=3, random_state=123)
tree_model.fit(simple_data[['square_feet', 'bedrooms']], simple_data['price'])

# Display results
print("Decision Tree Model:")
print(f"R-squared: {r2_score(simple_data['price'], tree_model.predict(simple_data[['square_feet', 'bedrooms']])):.3f}")

# Plot the tree
plt.figure(figsize=(12, 8))
plot_tree(tree_model, feature_names=['square_feet', 'bedrooms'], 
          filled=True, rounded=True, fontsize=10)
plt.title('Simple Decision Tree Example')
plt.show()
```

:::

### 2. Feature Importance: Understanding What Matters

Now let's see how feature importance works:

::: {.panel-tabset}

### R

```{r}
#| label: feature-importance-r
#| fig-cap: R feature importance example
#| echo: true

suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(rpart))

# Create example with clear feature importance
set.seed(456)
importance_data <- tibble(
  important_var = rnorm(100, mean = 10, sd = 2),
  less_important_var = rnorm(100, mean = 5, sd = 1),
  noise_var = rnorm(100, mean = 0, sd = 1),
  target = 2 * important_var + 0.5 * less_important_var + 0.1 * noise_var + rnorm(100, 0, 1)
)

# Fit decision tree
tree_model <- rpart(target ~ important_var + less_important_var + noise_var, 
                   data = importance_data)

# Extract feature importance
importance_scores <- tree_model$variable.importance
importance_df <- tibble(
  variable = names(importance_scores),
  importance = as.numeric(importance_scores)
) %>%
  arrange(desc(importance))

# Display feature importance
print("Feature Importance Rankings:")
importance_df

# Create visualization
ggplot(importance_df, aes(x = reorder(variable, importance), y = importance)) +
  geom_col(fill = "steelblue", alpha = 0.7) +
  coord_flip() +
  labs(
    title = "Feature Importance in Decision Tree",
    x = "Variables",
    y = "Importance Score"
  ) +
  theme_minimal()
```

### Python

```{python}
#| label: feature-importance-python
#| fig-cap: Python feature importance example
#| echo: true

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import r2_score

# Set seed for reproducibility
np.random.seed(456)

# Create example with clear feature importance
n = 100
important_var = np.random.normal(10, 2, n)
less_important_var = np.random.normal(5, 1, n)
noise_var = np.random.normal(0, 1, n)
target = 2 * important_var + 0.5 * less_important_var + 0.1 * noise_var + np.random.normal(0, 1, n)

# Create DataFrame
importance_data = pd.DataFrame({
    'important_var': important_var,
    'less_important_var': less_important_var,
    'noise_var': noise_var,
    'target': target
})

# Fit decision tree
tree_model = DecisionTreeRegressor(random_state=456)
tree_model.fit(importance_data[['important_var', 'less_important_var', 'noise_var']], 
               importance_data['target'])

# Extract feature importance
importance_scores = tree_model.feature_importances_
feature_names = ['important_var', 'less_important_var', 'noise_var']

# Create DataFrame
importance_df = pd.DataFrame({
    'variable': feature_names,
    'importance': importance_scores
}).sort_values('importance', ascending=True)

# Display feature importance
print("Feature Importance Rankings:")
print(importance_df)

# Create visualization
plt.figure(figsize=(10, 6))
plt.barh(importance_df['variable'], importance_df['importance'], color='steelblue', alpha=0.7)
plt.title('Feature Importance in Decision Tree')
plt.xlabel('Importance Score')
plt.ylabel('Variables')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

:::

### 3. Categorical vs. Numerical Encoding: The Key Difference

Let's see how encoding affects feature importance:

::: {.panel-tabset}

### R

```{r}
#| label: encoding-comparison-r
#| fig-cap: R categorical vs numerical encoding comparison
#| echo: true

suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(rpart))

# Create example with categorical variable
set.seed(789)
encoding_data <- tibble(
  neighborhood = sample(c("A", "B", "C", "D"), 200, replace = TRUE),
  square_feet = rnorm(200, mean = 1500, sd = 300),
  price = case_when(
    neighborhood == "A" ~ 200000 + 50 * square_feet + rnorm(200, 0, 10000),
    neighborhood == "B" ~ 180000 + 50 * square_feet + rnorm(200, 0, 10000),
    neighborhood == "C" ~ 160000 + 50 * square_feet + rnorm(200, 0, 10000),
    neighborhood == "D" ~ 140000 + 50 * square_feet + rnorm(200, 0, 10000)
  )
) %>%
  mutate(
    neighborhood_numeric = as.numeric(factor(neighborhood))
  )

# Model 1: Proper encoding (categorical as factor)
tree_proper <- rpart(price ~ neighborhood + square_feet, data = encoding_data)

# Model 2: Improper encoding (categorical as numeric)
tree_improper <- rpart(price ~ neighborhood_numeric + square_feet, data = encoding_data)

# Compare feature importance
proper_importance <- tibble(
  variable = names(tree_proper$variable.importance),
  importance = as.numeric(tree_proper$variable.importance),
  model = "Proper Encoding"
)

improper_importance <- tibble(
  variable = names(tree_improper$variable.importance),
  importance = as.numeric(tree_improper$variable.importance),
  model = "Improper Encoding"
)

# Combine and compare
comparison_df <- bind_rows(proper_importance, improper_importance)

print("Feature Importance Comparison:")
comparison_df

# Create comparison visualization
ggplot(comparison_df, aes(x = variable, y = importance, fill = model)) +
  geom_col(position = "dodge", alpha = 0.7) +
  labs(
    title = "Feature Importance: Proper vs. Improper Encoding",
    x = "Variables",
    y = "Importance Score",
    fill = "Encoding Method"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

### Python

```{python}
#| label: encoding-comparison-python
#| fig-cap: Python categorical vs numerical encoding comparison
#| echo: true

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor
from sklearn.preprocessing import LabelEncoder

# Set seed for reproducibility
np.random.seed(789)

# Create example with categorical variable
n = 200
neighborhoods = np.random.choice(['A', 'B', 'C', 'D'], n)
square_feet = np.random.normal(1500, 300, n)

# Create price based on neighborhood (categorical effect)
price = np.where(neighborhoods == 'A', 200000 + 50 * square_feet + np.random.normal(0, 10000, n),
                np.where(neighborhoods == 'B', 180000 + 50 * square_feet + np.random.normal(0, 10000, n),
                        np.where(neighborhoods == 'C', 160000 + 50 * square_feet + np.random.normal(0, 10000, n),
                                140000 + 50 * square_feet + np.random.normal(0, 10000, n))))

# Create DataFrame
encoding_data = pd.DataFrame({
    'neighborhood': neighborhoods,
    'square_feet': square_feet,
    'price': price
})

# Encode neighborhood as numeric
le = LabelEncoder()
encoding_data['neighborhood_numeric'] = le.fit_transform(encoding_data['neighborhood'])

# Model 1: Proper encoding (categorical as string)
tree_proper = DecisionTreeRegressor(random_state=789)
tree_proper.fit(encoding_data[['neighborhood', 'square_feet']], encoding_data['price'])

# Model 2: Improper encoding (categorical as numeric)
tree_improper = DecisionTreeRegressor(random_state=789)
tree_improper.fit(encoding_data[['neighborhood_numeric', 'square_feet']], encoding_data['price'])

# Compare feature importance
proper_importance = pd.DataFrame({
    'variable': ['neighborhood', 'square_feet'],
    'importance': tree_proper.feature_importances_,
    'model': 'Proper Encoding'
})

improper_importance = pd.DataFrame({
    'variable': ['neighborhood_numeric', 'square_feet'],
    'importance': tree_improper.feature_importances_,
    'model': 'Improper Encoding'
})

# Combine and compare
comparison_df = pd.concat([proper_importance, improper_importance], ignore_index=True)

print("Feature Importance Comparison:")
print(comparison_df)

# Create comparison visualization
plt.figure(figsize=(10, 6))
x_pos = np.arange(len(comparison_df['variable'].unique()))
width = 0.35

proper_scores = comparison_df[comparison_df['model'] == 'Proper Encoding']['importance'].values
improper_scores = comparison_df[comparison_df['model'] == 'Improper Encoding']['importance'].values

plt.bar(x_pos - width/2, proper_scores, width, label='Proper Encoding', alpha=0.7)
plt.bar(x_pos + width/2, improper_scores, width, label='Improper Encoding', alpha=0.7)

plt.xlabel('Variables')
plt.ylabel('Importance Score')
plt.title('Feature Importance: Proper vs. Improper Encoding')
plt.xticks(x_pos, comparison_df['variable'].unique(), rotation=45)
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

:::

::: {.callout-note}
## Understanding Feature Importance in Decision Trees

**How Feature Importance is Calculated:**

1. **Impurity Reduction:** For each split, the tree calculates how much the split reduces impurity (variance for regression, gini/entropy for classification)
2. **Weighted Average:** Feature importance is the weighted average of impurity reduction across all splits using that feature
3. **Normalization:** Importance scores are normalized so they sum to 1.0

**Why Encoding Matters:**

- **Categorical as Factor:** Tree can make meaningful splits like "Neighborhood in {A, B}" vs "Neighborhood in {C, D}"
- **Categorical as Numeric:** Tree might split on "Neighborhood > 2.5", which doesn't make sense for categorical data
- **Feature Importance:** Improper encoding can make categorical variables appear less important than they really are

**The Key Insight:** Proper encoding preserves the true nature of categorical variables and gives us accurate feature importance rankings that reflect real-world relationships.
:::

## The Decision Tree Challenge: A Deeper Look

The "feature importance and encoding" problem occurs when we don't properly encode categorical variables, leading to misleading insights about which features matter most. This happens in several ways:

1. **False numerical order:** Treating categorical variables as if they have meaningful numerical relationships
2. **Suboptimal splits:** Trees make splits that don't reflect the true categorical nature of variables
3. **Misleading importance:** Feature importance rankings that don't reflect real-world relationships
4. **Poor interpretability:** Models that are hard to understand and explain to stakeholders

### Why This Matters

In the real world, improper encoding can lead to:

- **False business insights:** Basing decisions on misleading feature importance rankings
- **Wasted resources:** Focusing on variables that appear important but aren't
- **Missed opportunities:** Ignoring truly important categorical variables
- **Loss of credibility:** When models don't make sense to domain experts

### The Solution

The key is to always ask:

1. **Is this variable truly categorical?** Does it represent discrete categories without inherent order?
2. **How should it be encoded?** Use proper categorical encoding (factors, one-hot, etc.)
3. **Do the splits make sense?** Can you interpret the tree splits in real-world terms?
4. **Are the importance rankings realistic?** Do they align with domain knowledge?
5. **Can you explain the model?** Is it interpretable to stakeholders?

Remember: **Proper encoding is crucial for interpretable machine learning!** üå≥
