<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.33">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Interpretable Data Models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="interpretableDataModels_files/libs/clipboard/clipboard.min.js"></script>
<script src="interpretableDataModels_files/libs/quarto-html/quarto.js" type="module"></script>
<script src="interpretableDataModels_files/libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="interpretableDataModels_files/libs/quarto-html/popper.min.js"></script>
<script src="interpretableDataModels_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="interpretableDataModels_files/libs/quarto-html/anchor.min.js"></script>
<link href="interpretableDataModels_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="interpretableDataModels_files/libs/quarto-html/quarto-syntax-highlighting-ea385d0e468b0dd5ea5bf0780b1290d9.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="interpretableDataModels_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="interpretableDataModels_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="interpretableDataModels_files/libs/bootstrap/bootstrap-bb462d781dde1847d9e3ccf7736099dd.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="quarto-light">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="interpretableDataModels.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li></ul></div></div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Interpretable Data Models</h1>
<p class="subtitle lead">Understanding the Why Behind Data-Driven Predictions</p>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="interpretable-data-models" class="level1">
<h1>üîç Interpretable Data Models</h1>
<section id="the-core-idea" class="level2">
<h2 class="anchored" data-anchor-id="the-core-idea">The Core Idea</h2>
<p>A data model is a function that represents how variables relate. In data science, models can take different functional forms:</p>
<div class="columns">
<div class="column" style="width:50%;">
<section id="direct-mappings" class="level3">
<h3 class="anchored" data-anchor-id="direct-mappings">Direct Mappings</h3>
<p><span class="math inline">\(f: X \rightarrow Y\)</span>, turning inputs into outputs.</p>
<p><strong>Example:</strong> Linear regression predicting house prices from square footage.</p>
</section>
</div><div class="column" style="width:50%;">
<section id="probability-distributions" class="level3">
<h3 class="anchored" data-anchor-id="probability-distributions">Probability Distributions</h3>
<p><span class="math inline">\(p(X,Y)\)</span>, describing how variables co-vary and with what uncertainty.</p>
<p><strong>Example:</strong> Bayesian networks modeling disease symptoms and causes.</p>
</section>
</div>
</div>
</section>
<section id="what-is-interpretability" class="level2">
<h2 class="anchored" data-anchor-id="what-is-interpretability">What is Interpretability?</h2>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
One-Liner Definition
</div>
</div>
<div class="callout-body-container callout-body">
<p>An <strong>interpretable data model</strong> is a function (like <span class="math inline">\(f(X)\)</span> or <span class="math inline">\(p(X,Y)\)</span>) whose structure is simple enough that humans can understand the relationships it encodes.</p>
</div>
</div>
<section id="expanded-definition" class="level3">
<h3 class="anchored" data-anchor-id="expanded-definition">Expanded Definition</h3>
<p>All models are functions, but their form varies. Sometimes they are explicit mappings from features to predictions (e.g., linear regression). Other times they are joint distributions over variables that describe uncertainty and dependencies (e.g., Bayesian networks).</p>
<p>An interpretable data model is one where this function is transparent enough that people can trace the ‚Äúwhy‚Äù behind its outputs. This usually means:</p>
<ul>
<li><strong>Clear structure</strong>: easily understood relationships (e.g., additive terms, decision rules)</li>
<li><strong>Human-scale parameters</strong>: coefficients, thresholds, or conditional probabilities that make intuitive sense<br>
</li>
<li><strong>Traceable reasoning</strong>: the ability to explain how a prediction or conclusion was reached</li>
</ul>
</section>
</section>
<section id="interpretability-vs-explainability" class="level2">
<h2 class="anchored" data-anchor-id="interpretability-vs-explainability">Interpretability vs Explainability</h2>
<div class="columns">
<div class="column" style="width:50%;">
<section id="interpretability" class="level3">
<h3 class="anchored" data-anchor-id="interpretability">Interpretability</h3>
<p><strong>Definition:</strong> The degree to which a model‚Äôs internal structure, parameters, and decision-making process can be understood by humans without requiring additional explanation tools.</p>
<p><strong>Focus:</strong> Built-in transparency of the model itself.</p>
</section>
</div><div class="column" style="width:50%;">
<section id="explainability" class="level3">
<h3 class="anchored" data-anchor-id="explainability">Explainability</h3>
<p><strong>Definition:</strong> The ability to provide clear, causal reasoning for why a model made a specific prediction, including the relative importance of different input features and the logical steps in the decision process.</p>
<p><strong>Focus:</strong> Post-hoc analysis and explanation of model decisions.</p>
<p><strong>Key Point:</strong> True explainability requires causal understanding‚Äînot just correlation. A good explanation answers ‚ÄúWhat would happen if we changed this input?‚Äù not just ‚ÄúWhat inputs were associated with this output?‚Äù</p>
</section>
</div>
</div>
<section id="the-causal-reasoning-challenge" class="level3">
<h3 class="anchored" data-anchor-id="the-causal-reasoning-challenge">The Causal Reasoning Challenge</h3>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Correlation vs.&nbsp;Causation in Explanations
</div>
</div>
<div class="callout-body-container callout-body">
<p>Most ‚Äúexplanations‚Äù from data models are actually just descriptions of correlations. True explainability requires causal reasoning:</p>
<ul>
<li><strong>Correlational explanation:</strong> ‚ÄúMaternal Tylenol use during pregnancy is associated with autism diagnosis in children‚Äù</li>
<li><strong>Causal explanation:</strong> ‚ÄúMaternal symptoms X, Y, and Z during pregnancy lead to both increased Tylenol use and increased autism risk in the child.‚Äù</li>
</ul>
<p>The difference matters for actionability and trust.</p>
</div>
</div>
</section>
</section>
<section id="why-it-matters" class="level2">
<h2 class="anchored" data-anchor-id="why-it-matters">Why It Matters</h2>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The Core Benefits
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>Trust</strong>: Users and stakeholders can validate and believe results</li>
<li><strong>Actionability</strong>: Insights can be acted on because the logic is clear<br>
</li>
<li><strong>Debuggability</strong>: Problems can be identified and corrected</li>
<li><strong>Ethics</strong>: Transparency helps detect bias and unintended consequences</li>
</ul>
</div>
</div>
<section id="the-problem-with-black-boxes" class="level3">
<h3 class="anchored" data-anchor-id="the-problem-with-black-boxes">The Problem with Black Boxes</h3>
<blockquote class="blockquote">
<p>‚ÄúThe problem is that a single metric, such as classification accuracy, is an incomplete description of most real-world tasks.‚Äù - Doshi-Velez and Kim (2017)</p>
</blockquote>
<p>When it comes to data modeling, you have to make a trade-off: Do you just want to know <strong>what</strong> is predicted? Or do you want to know <strong>why</strong> the prediction was made and possibly pay for the interpretability with a drop in predictive performance?</p>
</section>
<section id="key-reasons-for-interpretability" class="level3">
<h3 class="anchored" data-anchor-id="key-reasons-for-interpretability">Key Reasons for Interpretability</h3>
</section>
<section id="human-curiosity-learning" class="level3">
<h3 class="anchored" data-anchor-id="human-curiosity-learning">Human Curiosity &amp; Learning</h3>
<p>Humans have a mental model of their environment that is updated when something unexpected happens. This update is performed by finding an explanation for the unexpected event.</p>
<p><strong>Example:</strong> A human feels unexpectedly sick and asks, ‚ÄúWhy do I feel so sick?‚Äù They learn that they get sick every time they eat those red berries. They update their mental model and decide that the berries caused the sickness.</p>
</section>
<section id="scientific-discovery" class="level3">
<h3 class="anchored" data-anchor-id="scientific-discovery">Scientific Discovery</h3>
<p>In many scientific disciplines, there is a change from qualitative to quantitative methods, and also towards data-driven modeling. The goal of science is to gain knowledge, but many problems are solved with big datasets and complex statistical models.</p>
<p><strong>The model itself becomes the source of knowledge instead of the data.</strong> Interpretability makes it possible to extract this additional knowledge captured by the model.</p>
</section>
<section id="safety-testing" class="level3">
<h3 class="anchored" data-anchor-id="safety-testing">Safety &amp; Testing</h3>
<p>Data models take on real-world tasks that require safety measures and testing.</p>
<p><strong>Example:</strong> A self-driving car automatically detects cyclists based on a computer vision system. You want to be 100% sure that the abstraction the system has learned is error-free because running over cyclists is very bad.</p>
</section>
<section id="bias-detection" class="level3">
<h3 class="anchored" data-anchor-id="bias-detection">Bias Detection</h3>
<p>By default, data models pick up biases from the training data. This could make your models discriminate against underrepresented groups.</p>
<p><strong>Interpretability is a useful debugging tool for detecting bias in data models.</strong></p>
</section>
<section id="social-acceptance" class="level3">
<h3 class="anchored" data-anchor-id="social-acceptance">Social Acceptance</h3>
<p>The process of integrating machines and algorithms into our daily lives requires interpretability to increase social acceptance.</p>
<p><strong>Example:</strong> Our vacuum cleaner ‚ÄúDustBot‚Äù gets stuck. As an explanation for the accident, DustBot tells us that it needs to be on an even surface. This creates shared meaning and trust.</p>
</section>
</section>
<section id="when-do-we-not-need-interpretability" class="level2">
<h2 class="anchored" data-anchor-id="when-do-we-not-need-interpretability">When Do We NOT Need Interpretability?</h2>
<section id="low-impact-scenarios" class="level3">
<h3 class="anchored" data-anchor-id="low-impact-scenarios">Low-Impact Scenarios</h3>
<p>Interpretability is not required if the consequences of being wrong are minimal or acceptable.</p>
<p><strong>Example:</strong> Sarah‚Äôs personal Spotify playlist predictor that suggests songs she might like based on her listening history. There‚Äôs no real problem if the model is wrong (at worst just a song she skips).</p>
</section>
<section id="well-studied-problems" class="level3">
<h3 class="anchored" data-anchor-id="well-studied-problems">Well-Studied Problems</h3>
<p>Some applications have been sufficiently well studied so that there is enough practical experience with the model.</p>
<p><strong>Example:</strong> Statistical models for optical character recognition that process images of envelopes and extract addresses. These systems have been in use for many years, and it‚Äôs clear that they work.</p>
</section>
<section id="gaming-prevention" class="level3">
<h3 class="anchored" data-anchor-id="gaming-prevention">Gaming Prevention</h3>
<p>Interpretability might enable people or programs to manipulate the system.</p>
<p><strong>Example:</strong> Facebook‚Äôs content ranking algorithm where users might manipulate their posts to game the system if they know which features affect visibility in their friends‚Äô feeds.</p>
</section>
</section>
<section id="what-makes-a-good-explanation" class="level2">
<h2 class="anchored" data-anchor-id="what-makes-a-good-explanation">What Makes a Good Explanation?</h2>
<section id="characteristics-of-human-friendly-explanations" class="level3">
<h3 class="anchored" data-anchor-id="characteristics-of-human-friendly-explanations">Characteristics of Human-Friendly Explanations</h3>
</section>
<section id="contrastive" class="level3">
<h3 class="anchored" data-anchor-id="contrastive">Contrastive</h3>
<p>Humans usually don‚Äôt ask why a certain prediction was made, but why this prediction was made <strong>instead of another prediction</strong>.</p>
<p><strong>Example:</strong> For a house price prediction, the house owner might be interested in why does this beautiful house seem so cheap?</p>
</section>
<section id="selected" class="level3">
<h3 class="anchored" data-anchor-id="selected">Selected</h3>
<p>People don‚Äôt expect explanations that cover the actual and complete list of causes of an event. We are used to selecting one or two causes from a variety of possible causes as THE explanation.</p>
<p><strong>What it means for data modeling:</strong> Make the explanation short, give only 1 to 3 reasons, even if the world is more complex.</p>
</section>
<section id="audience-focused" class="level3">
<h3 class="anchored" data-anchor-id="audience-focused">Audience-focused</h3>
<p>Explanations are part of a conversation or interaction between the explainer and the receiver of the explanation.</p>
<p><strong>Example:</strong> Explaining cryptocurrencies to a technical person vs.&nbsp;to your grandmother would be completely different.</p>
</section>
<section id="focus-on-abnormal" class="level3">
<h3 class="anchored" data-anchor-id="focus-on-abnormal">Focus on Abnormal</h3>
<p>People focus more on abnormal causes to explain events. These are causes or effects that were not expected but nevertheless happened.</p>
<p><strong>Example:</strong> Viagra was originally developed as a heart medication, but researchers discovered its unexpected side effect during clinical trials. This ‚Äúabnormal‚Äù outcome became the primary explanation for the drug‚Äôs success, overshadowing its original intended purpose.</p>
</section>
<section id="truthful" class="level3">
<h3 class="anchored" data-anchor-id="truthful">Truthful</h3>
<p>Good explanations prove to be true in reality (i.e., in other situations).</p>
<p><strong>Example:</strong> GPS satellites need to account for time dilation effects from Einstein‚Äôs theory of relativity. While Newtonian physics worked well for centuries, it became inadequate for GPS precision. The satellites‚Äô clocks run faster in orbit due to weaker gravity, and this relativistic effect would cause GPS to be off by about 11 kilometers per day if not corrected.</p>
<p><strong>For data modeling:</strong> The explanation should predict the event as truthfully as possible, which in data modeling is sometimes called <strong>fidelity</strong>.</p>
</section>
<section id="consistent-with-prior-beliefs" class="level3">
<h3 class="anchored" data-anchor-id="consistent-with-prior-beliefs">Consistent with Prior Beliefs</h3>
<p>Humans often judge arguments by whether the conclusion fits their beliefs, rather than whether both their assumptions and their the reasoning are logically valid. This is called the belief bias effect.</p>
<p><strong>Classic experiment (Evans, Barston &amp; Pollard, 1983):</strong> Participants evaluated syllogisms like:</p>
<p>Premise 1: All mammals can walk.</p>
<p>Premise 2: Whales are mammals.</p>
<p>Conclusion: Whales can walk.</p>
<p>Although the conclusion is logically valid, it is completely unbeleivable. Despite this, approximately 50% of participants would judge conclusions like this as true.</p>
<p><strong>The dark side:</strong> While belief consistency makes explanations more ‚Äúhuman-friendly,‚Äù it‚Äôs arguably not very friendly to our future as a society. We‚Äôre essentially designing systems that reinforce existing biases rather than challenging them.</p>
<p><strong>Implication for data modeling:</strong> Users may dismiss explanations from a model if they contradict their existing mental models, even when the model is correct. Conversely, users may accept explanations that align with their beliefs, even when the model is wrong or the reasoning is flawed.</p>
</section>
</section>
<section id="example-loan-approval-model" class="level2">
<h2 class="anchored" data-anchor-id="example-loan-approval-model">Example: Loan Approval Model</h2>
<p>Let‚Äôs consider a practical example of interpretability in action:</p>
<section id="the-problem" class="level3">
<h3 class="anchored" data-anchor-id="the-problem">The Problem</h3>
<p>A bank uses a statistical model to approve or reject loan applications. The model performs well on test data, but applicants (and regulators) want to understand why decisions are made.</p>
</section>
<section id="without-interpretability" class="level3">
<h3 class="anchored" data-anchor-id="without-interpretability">Without Interpretability</h3>
<ul>
<li><strong>Applicant:</strong> ‚ÄúWhy was my loan rejected?‚Äù</li>
<li><strong>Bank:</strong> ‚ÄúThe model said no.‚Äù</li>
<li><strong>Result:</strong> Frustration, lack of trust, potential discrimination</li>
</ul>
</section>
<section id="with-interpretability" class="level3">
<h3 class="anchored" data-anchor-id="with-interpretability">With Interpretability</h3>
<ul>
<li><strong>Applicant:</strong> ‚ÄúWhy was my loan rejected?‚Äù</li>
<li><strong>Bank:</strong> ‚ÄúYour application was rejected because your debt-to-income ratio is 45% (our threshold is 40%) and you have two recent late payments. If you can reduce your debt-to-income ratio to below 40% and maintain on-time payments for 6 months, you would likely be approved.‚Äù</li>
<li><strong>Result:</strong> Clear guidance, trust, actionable steps</li>
</ul>
</section>
<section id="legal-requirements-for-interpretability" class="level3">
<h3 class="anchored" data-anchor-id="legal-requirements-for-interpretability">Legal Requirements for Interpretability</h3>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Banks Must Provide Interpretability
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Equal Credit Opportunity Act (ECOA)</strong> - Requires lenders to provide ‚Äúadverse action notices‚Äù that explain why credit was denied, including the specific reasons for the decision.</p>
</div>
</div>
<p>This means that in your loan example, the bank <strong>legally cannot</strong> just say ‚ÄúThe model said no.‚Äù They must provide interpretable explanations of the decision factors.</p>
</section>
</section>
<section id="examples" class="level2">
<h2 class="anchored" data-anchor-id="examples">Examples</h2>
<div class="columns">
<div class="column" style="width:50%;">
<section id="interpretable-models" class="level3">
<h3 class="anchored" data-anchor-id="interpretable-models">Interpretable Models</h3>
<ul>
<li><strong>Linear regression</strong>: Clear coefficients and additive structure</li>
<li><strong>Decision trees</strong>: Simple if-then rules</li>
<li><strong>Simple Bayesian models</strong>: Transparent conditional probabilities</li>
<li><strong>Rule-based systems</strong>: Explicit logical statements</li>
</ul>
</section>
</div><div class="column" style="width:50%;">
<section id="less-interpretable-models" class="level3">
<h3 class="anchored" data-anchor-id="less-interpretable-models">Less Interpretable Models</h3>
<ul>
<li><strong>Deep neural networks</strong>: Complex non-linear transformations</li>
<li><strong>Ensemble models</strong>: Random forests, gradient boosting (without explanation tools)</li>
<li><strong>Support vector machines</strong>: High-dimensional transformations</li>
<li><strong>Complex statistical models</strong>: Many interactions and non-linearities</li>
</ul>
</section>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
In Short
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Interpretability means models aren‚Äôt just accurate ‚Äî they‚Äôre understandable.</strong></p>
</div>
</div>
</section>
<section id="linear-regression-as-canonical-interpretable-model-example" class="level2">
<h2 class="anchored" data-anchor-id="linear-regression-as-canonical-interpretable-model-example">Linear Regression as Canonical Interpretable Model Example</h2>
<p>BuildIt Inc.&nbsp;flips houses - they buy houses to quickly renovate and then sell. Their specialty is building additions on to existing houses in established neighborhoods and then selling the home at prices above their total investment. BuildIt‚Äôs decision to move into a neighborhood is based on how sales price fluctuates with square footage. If sales price seems to increase by more than $120 per additional square foot, then they consider that neighborhood to be a good candidate for buying houses. BuildIt is eyeing a new neighborhood and records the square footage and prices of some recent sales transactions.</p>
<p>Here is the <code>python</code> code that creates a dataframe with BuildIt‚Äôs data (note: <code>salesPrice</code> is in thousands of dollars):</p>
<div id="172569c7" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">## make two data arrays representing observations</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co">## of six home sales</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>salesPrice <span class="op">=</span> [<span class="dv">160</span>, <span class="dv">220</span>, <span class="dv">190</span>, <span class="dv">250</span>, <span class="dv">290</span>, <span class="dv">240</span>]</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>sqFootage <span class="op">=</span> [<span class="dv">960</span>, <span class="dv">1285</span>, <span class="dv">1350</span>, <span class="dv">1600</span>, <span class="dv">1850</span>, <span class="dv">1900</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Visually, we can confirm what appears to be a linear relationship between square footage and sales prices by creating a scatterplot with a linear regression line drawn in blue:</p>
<div id="cell-fig-firstPlot" class="cell" data-message="false" data-results="hide" data-execution_count="2">
<div class="cell-output cell-output-display">
<div id="fig-firstplot" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-firstplot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="interpretableDataModels_files/figure-html/fig-firstplot-output-1.png" width="623" height="463" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-firstplot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: A plaubile regression line placed through the six observed sales.
</figcaption>
</figure>
</div>
</div>
</div>
<p>BuildIt is interested in the slope of this line which gives the estimated change in mean sales price for each unit change in square footage. For BuildIt, they want to know if this slope is above $120 per square foot because at that price point the firm is confident it can make money?</p>
<p>Letting,</p>
<p><span class="math display">\[
\begin{aligned}
x_i \equiv &amp;\textrm{ The square footage for the } i^{th} \textrm{ house.} \\
y_i \equiv &amp;\textrm{ The observed sales price, in 000's, for the } i^{th} \textrm{ house.} \\
\alpha \equiv &amp;\textrm{ The intercept term for the regression line.} \\
\beta \equiv &amp;\textrm{ The slope coefficient representing change in expected price per square footage.} \\
\mu_i \equiv &amp;\textrm{ The expected sales price, in 000's, for any given square footage where } \\
&amp; \hspace{0.2cm} \mu_i = E(y_i | x_i) \textrm{ and } \mu_i = \alpha + \beta \times x_i.
\end{aligned}
\]</span></p>
<p>the linear regression output can be extracted code in <code>R</code> or <code>python</code>. We will not worry about the coding details here. In the below code output, the first element of the returned array is <span class="math inline">\(\beta\)</span>, the slope coefficient for how price changes per square foot and the second element of the array is <span class="math inline">\(\alpha\)</span> typically referred to as the y-intercept.</p>
<div id="939b1a9d" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>np.polyfit(x <span class="op">=</span> sqFootage, y <span class="op">=</span> salesPrice, deg <span class="op">=</span> <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>array([ 0.1114099, 58.9064047])</code></pre>
</div>
</div>
<p>Based on the output, the following linear equation is the so-called ‚Äúbest‚Äù line:</p>
<p><span class="math display">\[
\mu_i = 58.91 + 0.1114 \times x_i
\]</span></p>
<p>The model suggests, assuming its assumptions are justified, that BuildIt can anticipate being able to sell additional square footage for about $111 per square foot (i.e.&nbsp;<span class="math inline">\(1000* \beta\)</span> because price is in 000‚Äôs). This would not earn them acceptable profit as it is less than $120 per square foot. However, with only <code>6</code> data points, there is obviously going to be tremendous uncertainty in this estimate.</p>
</section>
<section id="visualizing-the-linear-model-as-a-generative-dag" class="level2">
<h2 class="anchored" data-anchor-id="visualizing-the-linear-model-as-a-generative-dag">Visualizing the Linear Model as a Generative DAG</h2>
<p>From previous coursework, you are probably familiar with simple linear regression equation expressed mathematically. This model can be visually expressed as a generative model with the assumption that our observed data is normally distributed around some line of expected sales prices. Let‚Äôs use the following notation to describe the line:</p>
<p><span class="math display">\[
\mu_i = \alpha + \beta x_i
\]</span></p>
<p>where,</p>
<p><span class="math display">\[
\begin{aligned}
x_i &amp;\equiv \textrm{The value of an explanatory variable for the } i^{th} \textrm{ observation.} \\
\alpha &amp;\equiv \textrm{ The intercept term for the line.} \\
\beta &amp;\equiv \textrm{ The slope coefficient for the line.} \\
\mu_i &amp;\equiv \textrm{ The expected value (or mean) for the } i^{th} \textrm{ observation.}
\end{aligned}
\]</span></p>
<p>Using a generative DAG, <strong>?@fig-lineDag</strong> presents a graphical version of simple linear regression.</p>
<div id="cell-fig-lineDag" class="cell" data-execution_count="5">
<div class="cell-output cell-output-display">
<div id="fig-linedag" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-linedag-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="interpretableDataModels_files/figure-html/fig-linedag-output-1.png" width="565" height="291" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-linedag-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Generative DAG model for linear regression.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The statistical model of the generative DAG in <strong>?@fig-lineDag</strong>, let‚Äôs digest the implied narrative. Starting at the bottom:</p>
<ul>
<li><em>Sales Price</em> Node(<span class="math inline">\(y\)</span>): We observe <em>Sales Price</em> data where each realization is normally distributed about an <em>Expected Sales Price</em>, <span class="math inline">\(\mu\)</span>.</li>
<li><em>Expected Sales Price</em> Node(<span class="math inline">\(\mu\)</span>): Each realization <span class="math inline">\(\mu\)</span> is actually a deterministic function of this node‚Äôs parents. Graphically, the double perimeter around the node signals this. This expectation varies with each observation. The only way this can happen is that it has a parent that varies with each observation; namely <em>Square Footage</em>.</li>
<li><em>Square Footage</em> Node(<span class="math inline">\(x\)</span>): The <em>Square Footage</em> is our model input (as noted by the darker fill); we just take the observed data as given.<br>
</li>
<li>All other yet-to-be discussed nodes are outside the <em>Observations</em> plate. Therefore, each prediction from the model will use a constant value for each of these nodes. The node we are most interested in is <em>Slope Coeff</em>, <span class="math inline">\(\beta\)</span>, as this is as an estimate of how home prices change when Build-It adds square footage. <em>Intercept</em> just sets some base-level home value and <em>Price Std. Dev.</em> gives a measure of how much home prices vary about the calculated expected price, <span class="math inline">\(\mu\)</span>.</li>
</ul>
</section>
<section id="further-reading-optional" class="level2">
<h2 class="anchored" data-anchor-id="further-reading-optional">Further Reading (optional)</h2>
<p>For a nice companion guide to interpretable machine learning methods and techniques, see:</p>
<p><strong>Molnar, C. (2023). Interpretable Machine Learning: A Guide for Making Black Box Models Explainable.</strong> Available at: https://christophm.github.io/interpretable-ml-book/</p>
</section>
</section>
<section id="examples-1" class="level1">
<h1>EXAMPLES</h1>
<section id="hrt-therapy-and-cardiac-disease-example" class="level2">
<h2 class="anchored" data-anchor-id="hrt-therapy-and-cardiac-disease-example">HRT Therapy and Cardiac Disease Example</h2>
<p>This example demonstrates how interpretable data models can help us understand complex relationships in medical data, particularly around hormone replacement therapy (HRT) and cardiac disease outcomes.</p>
<section id="the-scenario" class="level3">
<h3 class="anchored" data-anchor-id="the-scenario">The Scenario</h3>
<p>Consider a study examining the relationship between hormone replacement therapy (HRT) and cardiac disease in women. A key challenge in this type of analysis is understanding the causal pathways and potential confounding variables.</p>
</section>
<section id="the-dag-model" class="level3">
<h3 class="anchored" data-anchor-id="the-dag-model">The DAG Model</h3>
<p>The following DAG illustrates the relationships between wealthy/healthy women, HRT therapy, and cardiac disease outcomes:</p>
<div id="cell-fig-hrtDag" class="cell" data-execution_count="7">
<div class="cell-output cell-output-display">
<div id="fig-hrtdag" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-hrtdag-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="interpretableDataModels_files/figure-html/fig-hrtdag-output-1.png" width="329" height="153" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-hrtdag-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: DAG showing relationships between wealthy/healthy women, HRT therapy, and cardiac disease outcomes.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="interpreting-the-dag" class="level3">
<h3 class="anchored" data-anchor-id="interpreting-the-dag">Interpreting the DAG</h3>
<p>The DAG in <strong>?@fig-hrtDag</strong> shows two important relationships:</p>
<ol type="1">
<li><strong>Wealthy/Healthy Women ‚Üí HRT Therapy</strong>: Women who are wealthy and healthy are more likely to receive HRT therapy. This could be due to:
<ul>
<li>Better access to healthcare</li>
<li>More proactive health management</li>
<li>Ability to afford treatments</li>
</ul></li>
<li><strong>Wealthy/Healthy Women ‚Üí Lower Cardiac Disease</strong>: Women who are wealthy and healthy have lower rates of cardiac disease. This relationship exists independently of HRT therapy and could be due to:
<ul>
<li>Better overall health status</li>
<li>Access to preventive care</li>
<li>Healthier lifestyle factors</li>
<li>Socioeconomic advantages</li>
</ul></li>
</ol>
</section>
<section id="why-this-matters-for-interpretability" class="level3">
<h3 class="anchored" data-anchor-id="why-this-matters-for-interpretability">Why This Matters for Interpretability</h3>
<p>This example illustrates a classic confounding scenario where:</p>
<ul>
<li><strong>Correlation ‚â† Causation</strong>: Simply observing that HRT users have lower cardiac disease rates doesn‚Äôt mean HRT causes the reduction</li>
<li><strong>Confounding Variable</strong>: Wealthy/healthy status is a confounder that affects both HRT usage and cardiac outcomes</li>
<li><strong>Interpretable Models Help</strong>: By explicitly modeling these relationships in a DAG, we can:
<ul>
<li>Identify potential confounding variables</li>
<li>Design studies that control for these factors</li>
<li>Make more accurate causal inferences</li>
<li>Avoid spurious conclusions</li>
</ul></li>
</ul>
</section>
<section id="key-takeaway" class="level3">
<h3 class="anchored" data-anchor-id="key-takeaway">Key Takeaway</h3>
<p>Interpretable models like DAGs help us move beyond simple correlations to understand the underlying causal structure of complex relationships. This is essential for making sound medical and policy decisions based on data analysis.</p>
<hr>
<p><em>‚ÄúThe most compelling analysts unify narrative, math, and code.‚Äù</em></p>
</section>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "Óßã";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>