---
title: "Decision Trees"
subtitle: "From Linear Slopes to Branching Logic"
format:
  html: default
  pdf: default
execute:
  echo: false
  eval: true
---

# 🌳 Decision Trees

## The Core Idea

Decision trees represent a fundamental shift from linear models to non-linear, rule-based approaches. While linear regression assumes relationships can be captured by straight lines, decision trees recognize that real-world relationships often require more flexible, branching logic.

::: {.columns}
::: {.column width="50%"}
### Linear Models
Assume relationships follow straight lines with constant slopes.

**Example:** Anxiety increases by 0.1 units for every minute of social media use.
:::

::: {.column width="50%"}
### Decision Trees
Split data into distinct groups based on threshold values.

**Example:** If social media time > 2 hours, then anxiety is high; otherwise, anxiety depends on other factors.
:::
:::

## The Anxiety Prediction Challenge

Our goal is to predict anxiety levels using available data. Let's start with the simplest approach and progressively add complexity to see how our predictions improve.

### The Anxiety and Social Media Dataset

Consider a study examining the relationship between social media usage and anxiety levels. We have data on time spent on social media (in hours) and stress survey responses, with corresponding anxiety levels measured by fMRI activity.

::: {.callout-important}
## 🔍 Understanding the True Relationship: Implied Coefficients

**Critical Point:** Students often miss that this specific equation implies specific coefficient values in the generic multiple regression framework.

**The Generic Multiple Regression Equation:**
$$
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon
$$

**In Our Case:**
$$
Anxiety = \beta_0 + \beta_1 \times Stress + \beta_2 \times Time + \epsilon
$$

**The True Coefficients (what we "know"):**

- $\beta_0 = 0$ (intercept is zero)
- $\beta_1 = 1$ (coefficient on Stress is 1)  
- $\beta_2 = 0.1$ (coefficient on Time is 0.1)

**Why This Matters:** When we run regression analysis, we're trying to estimate these $\beta$ coefficients. If our regression gives us coefficients that are very different from these true values, we know our model is wrong—even if it has good statistical fit!
:::

```{python}
#| echo: false
#| include: false
#| results: hide
import matplotlib.pyplot as plt
import pandas as pd
from functools import partial, partialmethod
import daft   ### %pip install -U git+https://github.com/daft-dev/daft.git
from numpy.random import default_rng
import numpy as np

class dag(daft.PGM):
    def __init__(self, *args, **kwargs):
        daft.PGM.__init__(self, *args, **kwargs)
    
    obsNode = partialmethod(daft.PGM.add_node, scale = 1.0, aspect = 3, fontsize = 9, plot_params = {'facecolor': 'cadetblue'})
    decNode = partialmethod(daft.PGM.add_node, aspect = 2.2, fontsize = 9, shape = "rectangle", plot_params = {'facecolor': 'thistle'})
    detNode = partialmethod(daft.PGM.add_node, scale = 1.0, aspect = 3, fontsize = 8.5, alternate = True, plot_params = {'facecolor': 'aliceblue'})
    latNode = partialmethod(daft.PGM.add_node, scale = 1.0, aspect = 3.45, fontsize = 8.5, plot_params = {'facecolor': 'aliceblue'})
    detNodeBig = partialmethod(daft.PGM.add_node, scale = 1.2, aspect = 2.25, fontsize = 9, alternate = True, plot_params = {'facecolor': 'aliceblue'})
    latNodeBig = partialmethod(daft.PGM.add_node, scale = 1.2, aspect = 2.2, fontsize = 9, plot_params = {'facecolor': 'aliceblue'})
    
pgm = dag(dpi = 100, alternate_style="outer")
pgm.latNode("y","Anxiety Level\n"+r"$y \sim Normal(\mu,\sigma)$",1,1)
pgm.detNode("mu","Expected Anxiety\n"+r"$\mu = \beta_0 + \beta_1 \times Stress + \beta_2 \times Time$",1,2, scale = 1.2, aspect = 5)
pgm.obsNode("stress","Baseline Stress\n"+r"$Stress$",-0,3, aspect = 3, scale = 1.2)
pgm.obsNode("time","Social Media Time\n"+r"$Time$",2,3, aspect = 3, scale = 1.2)
pgm.latNode("beta0","Intercept\n"+r"$\beta_0 = 0$", -2, 2)
pgm.latNode("beta1","Stress Coefficient\n"+r"$\beta_1 = 1$", -2, 3)
pgm.latNode("beta2","Time Coefficient\n"+r"$\beta_2 = 0.1$", 4, 3)
pgm.latNode("sigma","Anxiety Std.Dev\n"+r"$\sigma = 0$", 4, 2)
pgm.add_edge("mu","y")
pgm.add_edge("stress","mu")
pgm.add_edge("time","mu")
pgm.add_edge("beta0","mu")
pgm.add_edge("beta1","mu")
pgm.add_edge("beta2","mu")
pgm.add_edge("sigma","y")
pgm.add_plate([-1, 0.2, 4, 3.25], label = "Observation:\n" + r"$i = 1, 2, \ldots, 15$", 
              label_offset = (2,2), rect_params = dict({"fill": False, "linestyle": "dashed", "edgecolor": "black"}))
```

```{python}
#| echo: false
#| label: fig-trueRelationshipDag
#| fig-cap: "True generative DAG model for anxiety prediction showing the correct relationship: Anxiety = Stress + 0.1 × Time"
#| out-width: 85%
pgm.show()
```

```{python}
#| echo: false
#| include: false
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.tree import DecisionTreeRegressor, plot_tree
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import warnings
warnings.filterwarnings('ignore')

# Set style for consistent plotting
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

# Generate the anxiety, social media time, and stress survey dataset
# This replicates the "garbage can regression" scenario where linear regression fails
np.random.seed(42)

# Create the dataset from the garbage can regression challenge
data = pd.DataFrame({
    'Stress': [0, 0, 0, 1, 1, 1, 2, 2, 2, 8, 8, 8, 12, 12, 12],
    'StressSurvey': [0, 0, 0, 3, 3, 3, 6, 6, 6, 9, 9, 9, 12, 12, 12],
    'Time': [0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2.1, 2.2, 2.2, 2.2],
    'Anxiety': [0, 0.1, 0.1, 1.1, 1.1, 1.1, 2.2, 2.2, 2.2, 8.2, 8.2, 8.21, 12.22, 12.22, 12.22]
})

# True relationship: Anxiety = Stress + 0.1 * Time
# But we'll use StressSurvey (non-linear proxy) instead of Stress (linear)
print("True relationship: Anxiety = Stress + 0.1 * Time")
print("But we observe StressSurvey (non-linear proxy) instead of Stress")
print("\nOur prediction challenge: Can we predict anxiety using Time and StressSurvey?")
print("\n📚 REMINDER: This dataset comes from the Garbage Can Regression Challenge!")
print("   We KNOW the true relationship: Anxiety = Stress + 0.1*Time")
print("   This means Time should have a POSITIVE coefficient of 0.1")
print("   But we only observe StressSurvey (non-linear proxy) instead of true Stress")
```

```{python}
#| label: tbl-anxiety-data
#| tbl-cap: "Anxiety, social media time, and stress survey dataset"
#| echo: true
data
```

### Step 1: Predicting Anxiety with Zero Variables (Baseline)

First, let's establish our baseline prediction using no independent variables:

```{python}
#| label: fig-baseline-prediction
#| fig-cap: "Baseline anxiety prediction using no independent variables"
#| echo: false

from scipy import stats

# Calculate baseline predictions
baseline_mean = data['Anxiety'].mean()
baseline_median = data['Anxiety'].median()

# Create dotplot with density function
fig, ax = plt.subplots(1, 1, figsize=(7.5, 5.5))

# Create dotplot
y_jitter = np.random.normal(0, 0.02, len(data['Anxiety']))
ax.scatter(data['Anxiety'], y_jitter, alpha=0.6, s=100, color='steelblue', 
           edgecolors='black', linewidth=1, label='Observed Anxiety Values')

# Add density curve
anxiety_range = np.linspace(data['Anxiety'].min() - 1, data['Anxiety'].max() + 1, 100)
density = stats.gaussian_kde(data['Anxiety'])
density_values = density(anxiety_range)
# Scale density to fit nicely on the plot
density_scaled = density_values / density_values.max() * 0.1
ax.fill_between(anxiety_range, -density_scaled, density_scaled, alpha=0.3, color='lightblue', label='Probability Density')

# Add mean and median lines
ax.axvline(baseline_mean, color='red', linestyle='-', linewidth=3, alpha=0.8, label=f'Mean = {baseline_mean:.2f}')
ax.axvline(baseline_median, color='green', linestyle='--', linewidth=3, alpha=0.8, label=f'Median = {baseline_median:.2f}')

# Add annotations
ax.annotate(f'Mean Prediction\n{baseline_mean:.2f}', 
            xy=(baseline_mean, 0.05), xytext=(baseline_mean + 1, 0.08),
            arrowprops=dict(arrowstyle='->', color='red', alpha=0.7),
            fontsize=12, ha='center', color='red', weight='bold')

ax.annotate(f'Median Prediction\n{baseline_median:.2f}', 
            xy=(baseline_median, -0.05), xytext=(baseline_median - 1, -0.08),
            arrowprops=dict(arrowstyle='->', color='green', alpha=0.7),
            fontsize=12, ha='center', color='green', weight='bold')

ax.set_xlabel('Anxiety Level', fontsize=12)
ax.set_ylabel('Density', fontsize=12)
ax.set_title('Baseline Anxiety Prediction: Mean vs Median', fontsize=14, weight='bold')
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3)
ax.set_ylim(-0.12, 0.12)

plt.tight_layout()
plt.show()

# Calculate errors for both predictions
mae_mean = np.mean(np.abs(data['Anxiety'] - baseline_mean))
mae_median = np.mean(np.abs(data['Anxiety'] - baseline_median))

print(f"Baseline Predictions (no variables):")
print(f"Mean prediction: {baseline_mean:.2f}")
print(f"Median prediction: {baseline_median:.2f}")
print(f"Mean Absolute Error (using mean): {mae_mean:.2f}")
print(f"Mean Absolute Error (using median): {mae_median:.2f}")
print(f"\nInterpretation: We predict everyone has anxiety level {baseline_mean:.2f} (mean) or {baseline_median:.2f} (median)")
print(f"This is our starting point before adding any independent variables.")
```

### Step 2: Predicting Anxiety with One Variable (Time)

Now let's see how much we can improve by using just social media time:

```{python}
#| label: fig-linear-relationship
#| fig-cap: "Predicting anxiety using social media time (one variable)"
#| echo: false

# Create figure
fig, ax = plt.subplots(1, 1, figsize=(7.5, 5.5))

# Scatter plot with regression line
ax.scatter(data['Time'], data['Anxiety'], 
           color='steelblue', s=100, alpha=0.7, edgecolors='black', linewidth=1)

# Fit linear regression
slope, intercept = np.polyfit(data['Time'], data['Anxiety'], 1)
line_x = np.linspace(0, 2.5, 100)
line_y = slope * line_x + intercept
ax.plot(line_x, line_y, 'r-', linewidth=3, label=f'Regression Line: y = {slope:.3f}x + {intercept:.3f}')

# Add slope visualization
x1, x2 = 1.0, 2.0
y1, y2 = slope * x1 + intercept, slope * x2 + intercept
ax.plot([x1, x2], [y1, y2], 'g-', linewidth=4, alpha=0.8, label='Slope = Rise/Run')
ax.plot([x1, x1], [y1, y2], 'g--', linewidth=2, alpha=0.6)
ax.plot([x1, x2], [y1, y1], 'g--', linewidth=2, alpha=0.6)

# Add slope annotation
ax.annotate(f'Rise = {y2-y1:.2f}', xy=(x1-0.1, (y1+y2)/2), 
            fontsize=12, ha='right', color='green', weight='bold')
ax.annotate(f'Run = {x2-x1}', xy=((x1+x2)/2, y1-0.5), 
            fontsize=12, ha='center', color='green', weight='bold')
ax.annotate(f'Slope = {slope:.3f}', xy=(x2+0.1, (y1+y2)/2), 
            fontsize=14, ha='left', color='red', weight='bold')

ax.set_xlabel('Social Media Time (hours)', fontsize=12)
ax.set_ylabel('Anxiety Level', fontsize=12)
ax.set_title('Bivariate Linear Regression: Time vs Anxiety', fontsize=14, weight='bold')
ax.legend(fontsize=10)
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Calculate prediction accuracy for one variable
time_predictions = slope * data['Time'] + intercept
time_mae = np.mean(np.abs(data['Anxiety'] - time_predictions))
baseline_prediction = data['Anxiety'].mean()  # Define baseline for R² calculation
time_r2 = 1 - np.sum((data['Anxiety'] - time_predictions)**2) / np.sum((data['Anxiety'] - baseline_prediction)**2)

print(f"One-Variable Prediction Results (Time only):")
print(f"Slope: {slope:.4f} (anxiety change per hour)")
print(f"Intercept: {intercept:.4f}")
print(f"Mean Absolute Error: {time_mae:.2f}")
print(f"R²: {time_r2:.3f}")
print(f"Interpretation: For every additional hour of social media use, anxiety changes by {slope:.4f} units")
print(f"True coefficient should be: 0.1 (positive!)")
print(f"Improvement over baseline: {((baseline_prediction - time_mae) / baseline_prediction * 100):.1f}% reduction in MAE")
```

### Step 3: Predicting Anxiety with Two Variables (Linear Regression)

Now let's see what happens when we add the StressSurvey variable to improve our anxiety predictions:

```{python}
#| label: fig-multiple-regression
#| fig-cap: "Multiple regression analysis showing the garbage can regression problem"
#| echo: false

from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
import statsmodels.api as sm

# Multiple regression: Anxiety ~ Time + StressSurvey
X = data[['Time', 'StressSurvey']]
y = data['Anxiety']

# Fit multiple regression
lr_multi = LinearRegression()
lr_multi.fit(X, y)
multi_predictions = lr_multi.predict(X)

# Get coefficients
time_coef = lr_multi.coef_[0]
stress_coef = lr_multi.coef_[1]
intercept = lr_multi.intercept_

# Create visualization - Plot 1: Actual vs Predicted
fig, ax = plt.subplots(1, 1, figsize=(7, 4))
ax.scatter(data['Anxiety'], multi_predictions, 
           color='steelblue', s=100, alpha=0.7, edgecolors='black', linewidth=1)
ax.plot([0, 15], [0, 15], 'r--', linewidth=2, alpha=0.8, label='Perfect Prediction')
ax.set_xlabel('Actual Anxiety', fontsize=12)
ax.set_ylabel('Predicted Anxiety', fontsize=12)
ax.set_title('Multiple Regression: Actual vs Predicted', fontsize=14, weight='bold')
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# Create visualization - Plot 2: Coefficient comparison
fig, ax = plt.subplots(1, 1, figsize=(7, 4))
coefficients = ['Time Coefficient', 'StressSurvey Coefficient']
true_values = [0.1, 1.0]  # True coefficients
estimated_values = [time_coef, stress_coef]

x = np.arange(len(coefficients))
width = 0.35

bars1 = ax.bar(x - width/2, true_values, width, label='True Coefficients', 
                color='lightgreen', alpha=0.8, edgecolor='black')
bars2 = ax.bar(x + width/2, estimated_values, width, label='Estimated Coefficients', 
                color='lightcoral', alpha=0.8, edgecolor='black')

ax.set_xlabel('Variables', fontsize=12)
ax.set_ylabel('Coefficient Value', fontsize=12)
ax.set_title('Coefficient Comparison: True vs Estimated', fontsize=14, weight='bold')
ax.set_xticks(x)
ax.set_xticklabels(coefficients)
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3, axis='y')

# Add value labels on bars
for bars in [bars1, bars2]:
    for bar in bars:
        height = bar.get_height()
        ax.annotate(f'{height:.3f}', xy=(bar.get_x() + bar.get_width()/2, height),
                    xytext=(0, 3), textcoords="offset points", ha='center', va='bottom',
                    fontsize=10, weight='bold')

plt.tight_layout()
plt.show()

# Calculate prediction accuracy for two variables (linear regression)
multi_mae = np.mean(np.abs(data['Anxiety'] - multi_predictions))
multi_r2 = r2_score(y, multi_predictions)

print("Two-Variable Prediction Results (Linear Regression):")
print("=" * 50)
print(f"Anxiety = {intercept:.3f} + {time_coef:.3f}*Time + {stress_coef:.3f}*StressSurvey")
print(f"Time coefficient: {time_coef:.4f} (should be +0.1)")
print(f"StressSurvey coefficient: {stress_coef:.4f} (should be +1.0)")
print(f"Mean Absolute Error: {multi_mae:.2f}")
print(f"R² = {multi_r2:.4f}")
print(f"Improvement over one variable: {((time_mae - multi_mae) / time_mae * 100):.1f}% reduction in MAE")
print(f"\n🚨 PROBLEM: Time coefficient is {time_coef:.4f} instead of +0.1!")
print("This is the 'garbage can regression' problem in action.")
```

::: {.callout-warning}
## The Garbage Can Regression Problem

The multiple regression shows a **negative coefficient for Time** when the true relationship should be **positive**! This happens because:

1. **StressSurvey is a non-linear proxy** for the true Stress variable
2. **Linear regression assumes linearity** but the relationship is non-linear
3. **The model compensates** by giving Time a negative coefficient to "correct" for the non-linear StressSurvey effect

**Our anxiety predictions are now misleading!** We're getting better R² but wrong interpretations. This is exactly why we need decision trees - they can capture these non-linear relationships without making false assumptions about linearity.
:::

## Step 4: Predicting Anxiety with Two Variables (Decision Trees)

Decision trees offer a fundamentally different approach to predicting anxiety. Instead of assuming linearity, they partition data into distinct groups based on threshold values, potentially giving us better and more interpretable anxiety predictions.

### What is a Decision Tree?

::: {.callout-note}
## One-Liner Definition

A **decision tree** is a model that splits data into groups using a series of binary decisions, where each split is based on a threshold value of a feature.
:::

### The Tree Structure

Decision trees consist of:

- **Root Node**: The starting point containing all data
- **Internal Nodes**: Decision points that split data based on conditions
- **Leaf Nodes**: Terminal nodes that provide predictions
- **Branches**: Paths connecting nodes based on decision outcomes

### Building a Decision Tree for Anxiety Prediction

Let's see how a decision tree would approach our anxiety prediction challenge using both Time and StressSurvey:

```{python}
#| label: fig-decision-tree-example
#| fig-cap: "Decision tree for anxiety prediction using Time and StressSurvey"
#| echo: false

# Create and fit decision tree with both features
tree_model = DecisionTreeRegressor(max_depth=2, random_state=42)
tree_model.fit(data[['Time', 'StressSurvey']], data['Anxiety'])

# Create visualization - Plot 1: Tree structure
fig, ax = plt.subplots(1, 1, figsize=(7, 4))
plot_tree(tree_model, feature_names=['Time', 'StressSurvey'], 
          filled=True, rounded=True, fontsize=10, ax=ax)
ax.set_title('Decision Tree Structure', fontsize=14, weight='bold')
plt.tight_layout()
plt.show()

# Create visualization - Plot 2: Tree predictions vs actual data
fig, ax = plt.subplots(1, 1, figsize=(7, 4))
ax.scatter(data['Anxiety'], tree_model.predict(data[['Time', 'StressSurvey']]), 
           color='steelblue', s=100, alpha=0.7, edgecolors='black', linewidth=1, 
           label='Tree Predictions')
ax.plot([0, 15], [0, 15], 'r--', linewidth=2, alpha=0.8, label='Perfect Prediction')
ax.set_xlabel('Actual Anxiety', fontsize=12)
ax.set_ylabel('Predicted Anxiety', fontsize=12)
ax.set_title('Decision Tree: Actual vs Predicted', fontsize=14, weight='bold')
ax.legend(fontsize=12)
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# Print tree rules
print("Decision Tree Rules:")
print("=" * 50)
tree_rules = []
def extract_rules(tree, feature_names, node=0, depth=0, rule=""):
    if tree.tree_.children_left[node] == tree.tree_.children_right[node]:  # Leaf node
        prediction = tree.tree_.value[node][0][0]
        tree_rules.append(f"{rule} → Anxiety = {prediction:.2f}")
    else:
        feature = feature_names[tree.tree_.feature[node]]
        threshold = tree.tree_.threshold[node]
        extract_rules(tree, feature_names, tree.tree_.children_left[node], depth+1, 
                     f"{rule} AND {feature} <= {threshold:.1f}" if rule else f"{feature} <= {threshold:.1f}")
        extract_rules(tree, feature_names, tree.tree_.children_right[node], depth+1, 
                     f"{rule} AND {feature} > {threshold:.1f}" if rule else f"{feature} > {threshold:.1f}")

extract_rules(tree_model, ['Time', 'StressSurvey'])
for i, rule in enumerate(tree_rules, 1):
    print(f"{i}. {rule}")

# Calculate prediction accuracy for two variables (decision tree)
tree_predictions = tree_model.predict(data[['Time', 'StressSurvey']])
tree_mae = np.mean(np.abs(data['Anxiety'] - tree_predictions))
tree_r2 = r2_score(data['Anxiety'], tree_predictions)

print(f"\nTwo-Variable Prediction Results (Decision Tree):")
print(f"Mean Absolute Error: {tree_mae:.2f}")
print(f"R² = {tree_r2:.4f}")
print(f"Improvement over linear regression: {((multi_mae - tree_mae) / multi_mae * 100):.1f}% reduction in MAE")
print(f"Improvement over baseline: {((baseline_prediction - tree_mae) / baseline_prediction * 100):.1f}% reduction in MAE")
```

### How Decision Trees Work: The CART Algorithm

The Classification and Regression Trees (CART) algorithm builds trees through a recursive process:

1. **Find Best Split**: For each feature, find the threshold that best separates the data
2. **Choose Best Feature**: Select the feature and threshold that minimize variance (regression) or Gini impurity (classification)
3. **Split Data**: Create two child nodes based on the chosen split
4. **Repeat**: Continue splitting until stopping criteria are met

::: {.callout-note}
## Computational Complexity of CART

**You're absolutely right!** CART does manually search over potential split points, which can be slow on large datasets:

- **For each feature**: CART considers every unique value as a potential split point
- **For each split point**: It calculates variance reduction (regression) or Gini impurity (classification)
- **Computational cost**: O(n × m × log n) where n = number of samples, m = number of features

**Why this matters:**
- **Large datasets**: With millions of rows, this becomes computationally expensive
- **Many features**: Each additional feature multiplies the search space
- **Modern alternatives**: Algorithms like Random Forest use sampling and parallelization to speed this up

**For our small anxiety dataset**: This is fast, but in real-world applications with large datasets, you'd want to use optimized implementations or ensemble methods.
:::

```{python}
#| label: fig-split-visualization
#| fig-cap: "Visualizing how decision trees find optimal splits using StressSurvey"
#| echo: false

# First, show the baseline (no split) - overall mean and total variance
fig, ax = plt.subplots(1, 1, figsize=(7, 5))
ax.scatter(data['StressSurvey'], data['Anxiety'], 
          color='lightgray', s=80, alpha=0.6, edgecolors='black')

# Calculate overall mean and variance
overall_mean = data['Anxiety'].mean()
overall_var = data['Anxiety'].var()

# Add overall mean line
ax.axhline(y=overall_mean, color='black', linewidth=3, linestyle='-', alpha=0.8)
ax.text(6, overall_mean + 0.5, f'Overall Mean: {overall_mean:.2f}', 
       ha='center', fontsize=12, weight='bold', color='black')

ax.set_title(f'No Split - Baseline\nTotal Variance: {overall_var:.3f}', 
            fontsize=12, weight='bold')
ax.set_xlabel('Stress Survey Score', fontsize=10)
ax.set_ylabel('Anxiety Level', fontsize=10)
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# Create individual plots for each split point
split_points = [3, 6, 9, 12]
colors = ['red', 'blue', 'green', 'orange']

for i, (split, color) in enumerate(zip(split_points, colors)):
    fig, ax = plt.subplots(1, 1, figsize=(7, 5))
    
    # Plot data points
    ax.scatter(data['StressSurvey'], data['Anxiety'], 
              color='lightgray', s=80, alpha=0.6, edgecolors='black')
    
    # Add split line
    ax.axvline(x=split, color=color, linewidth=3, linestyle='--', alpha=0.8)
    
    # Calculate means for each group
    left_group = data[data['StressSurvey'] <= split]
    right_group = data[data['StressSurvey'] > split]
    
    if len(left_group) > 0:
        left_mean = left_group['Anxiety'].mean()
        ax.axhline(y=left_mean, xmin=0, xmax=split/12, color=color, linewidth=2)
        ax.text(split/2, left_mean + 0.5, f'Mean: {left_mean:.2f}', 
               ha='center', fontsize=10, weight='bold', color=color)
    
    if len(right_group) > 0:
        right_mean = right_group['Anxiety'].mean()
        ax.axhline(y=right_mean, xmin=split/12, xmax=1, color=color, linewidth=2)
        ax.text((split + 12)/2, right_mean + 0.5, f'Mean: {right_mean:.2f}', 
               ha='center', fontsize=10, weight='bold', color=color)
    
    # Calculate variance reduction
    if len(left_group) > 0 and len(right_group) > 0:
        total_var = data['Anxiety'].var()
        left_var = left_group['Anxiety'].var() * len(left_group) / len(data)
        right_var = right_group['Anxiety'].var() * len(right_group) / len(data)
        variance_reduction = total_var - (left_var + right_var)
        
        ax.set_title(f'Split at StressSurvey = {split}\nVariance Reduction: {variance_reduction:.3f}', 
                    fontsize=12, weight='bold')
    
    ax.set_xlabel('Stress Survey Score', fontsize=10)
    ax.set_ylabel('Anxiety Level', fontsize=10)
    ax.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()
```

## Comparing Our Anxiety Prediction Approaches

### Summary: How Well Do We Predict Anxiety?

Let's compare all our approaches to predicting anxiety:

```{python}
#| label: fig-model-comparison
#| fig-cap: "Comparing how linear regression and decision trees handle the Time effect"
#| echo: false

# Calculate metrics for both models
lr_mse = mean_squared_error(data['Anxiety'], multi_predictions)
tree_mse = mean_squared_error(data['Anxiety'], tree_predictions)
lr_r2 = r2_score(data['Anxiety'], multi_predictions)

# Create comparison plot - Plot 1: Actual vs Predicted comparison
fig, ax = plt.subplots(1, 1, figsize=(7, 4))
ax.scatter(data['Anxiety'], multi_predictions, 
           color='blue', s=100, alpha=0.7, edgecolors='black', linewidth=1, 
           label=f'Multiple Regression (R² = {lr_r2:.3f})')
ax.scatter(data['Anxiety'], tree_predictions, 
           color='red', s=100, alpha=0.7, edgecolors='black', linewidth=1, 
           label=f'Decision Tree (R² = {tree_r2:.3f})')
ax.plot([0, 15], [0, 15], 'k--', linewidth=2, alpha=0.8, label='Perfect Prediction')

ax.set_xlabel('Actual Anxiety', fontsize=12)
ax.set_ylabel('Predicted Anxiety', fontsize=12)
ax.set_title('Model Predictions Comparison', fontsize=14, weight='bold')
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# Create comparison plot - Plot 2: Performance metrics
fig, ax = plt.subplots(1, 1, figsize=(7, 4))
models = ['Multiple Regression', 'Decision Tree']
mse_values = [lr_mse, tree_mse]
r2_values = [lr_r2, tree_r2]

x = np.arange(len(models))
width = 0.35

bars1 = ax.bar(x - width/2, mse_values, width, label='MSE', color='lightcoral', alpha=0.8)
bars2 = ax.bar(x + width/2, r2_values, width, label='R²', color='lightblue', alpha=0.8)

ax.set_xlabel('Model Type', fontsize=12)
ax.set_ylabel('Score', fontsize=12)
ax.set_title('Performance Metrics Comparison', fontsize=14, weight='bold')
ax.set_xticks(x)
ax.set_xticklabels(models)
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3, axis='y')

# Add value labels on bars
for bars in [bars1, bars2]:
    for bar in bars:
        height = bar.get_height()
        ax.annotate(f'{height:.3f}', xy=(bar.get_x() + bar.get_width()/2, height),
                    xytext=(0, 3), textcoords="offset points", ha='center', va='bottom',
                    fontsize=10, weight='bold')

plt.tight_layout()
plt.show()

print("Anxiety Prediction Summary:")
print("=" * 50)
print(f"0 Variables (Baseline):     MAE: {np.mean(np.abs(data['Anxiety'] - baseline_prediction)):.2f}")
print(f"1 Variable (Time only):     MAE: {time_mae:.2f}")
print(f"2 Variables (Linear Reg):   MAE: {multi_mae:.2f}")
print(f"2 Variables (Decision Tree): MAE: {tree_mae:.2f}")
print(f"\nKey Insights:")
print(f"• Adding Time improves predictions by {((baseline_prediction - time_mae) / baseline_prediction * 100):.1f}%")
print(f"• Adding StressSurvey (linear) improves by {((time_mae - multi_mae) / time_mae * 100):.1f}% more")
print(f"• Decision trees improve by {((multi_mae - tree_mae) / multi_mae * 100):.1f}% over linear regression")
print(f"• Decision trees can capture the true positive Time effect without being misled")
print(f"  by the non-linear StressSurvey relationship!")
```

### How is Feature Importance Calculated?

Before we compare feature importance, let's understand how decision trees calculate it:

::: {.callout-note}
## Feature Importance Calculation in Decision Trees

**The basic idea**: Feature importance measures how much each feature contributes to reducing prediction error across all splits in the tree.

**Step-by-step calculation**:

1. **For each split**: Calculate how much variance/impurity is reduced by that split
2. **Weight by samples**: Multiply by the number of samples that go through that split
3. **Sum by feature**: Add up all the weighted reductions for each feature
4. **Normalize**: Divide by the total reduction to get proportions that sum to 1
:::

Let's see this in action with our anxiety prediction tree:

```{python}
#| label: fig-feature-importance-calculation
#| fig-cap: "Step-by-step calculation of feature importance in decision trees"
#| echo: false

def calculate_feature_importance_manually(tree_model, feature_names):
    """Manually calculate feature importance to show the process"""
    tree = tree_model.tree_
    
    # Initialize importance for each feature
    feature_importance = np.zeros(len(feature_names))
    
    print("Feature Importance Calculation Process:")
    print("=" * 50)
    
    # Go through each node in the tree
    for i in range(tree.node_count):
        if tree.feature[i] != -2:  # Not a leaf node
            feature_idx = tree.feature[i]
            feature_name = feature_names[feature_idx]
            
            # Get the number of samples at this node
            n_samples = tree.n_node_samples[i]
            
            # Calculate weighted impurity reduction
            # This is simplified - in practice, it uses the actual impurity measure
            left_samples = tree.n_node_samples[tree.children_left[i]]
            right_samples = tree.n_node_samples[tree.children_right[i]]
            
            # Simplified impurity reduction (using sample counts as proxy)
            impurity_reduction = n_samples * (1 - (left_samples + right_samples) / n_samples)
            
            # Add to feature importance
            feature_importance[feature_idx] += impurity_reduction
            
            print(f"Node {i}: Split on {feature_name} <= {tree.threshold[i]:.1f}")
            print(f"  Samples: {n_samples}, Left: {left_samples}, Right: {right_samples}")
            print(f"  Impurity reduction: {impurity_reduction:.2f}")
            print(f"  Running total for {feature_name}: {feature_importance[feature_idx]:.2f}")
            print()
    
    # Normalize to sum to 1
    total_importance = np.sum(feature_importance)
    if total_importance > 0:
        feature_importance = feature_importance / total_importance
    
    return feature_importance

# Calculate manually
manual_importance = calculate_feature_importance_manually(tree_model, ['Time', 'StressSurvey'])

# Compare with sklearn's calculation
sklearn_importance = tree_model.feature_importances_

print("Comparison of Manual vs Sklearn Calculation:")
print("=" * 50)
for i, feature in enumerate(['Time', 'StressSurvey']):
    print(f"{feature}:")
    print(f"  Manual calculation: {manual_importance[i]:.3f}")
    print(f"  Sklearn calculation: {sklearn_importance[i]:.3f}")
    print(f"  Difference: {abs(manual_importance[i] - sklearn_importance[i]):.3f}")
    print()

# Visualize the calculation process - Plot 1: Tree structure
fig, ax = plt.subplots(1, 1, figsize=(7, 4))
plot_tree(tree_model, feature_names=['Time', 'StressSurvey'], 
          filled=True, rounded=True, fontsize=10, ax=ax)
ax.set_title('Decision Tree with Node Information', fontsize=14, weight='bold')
plt.tight_layout()
plt.show()

# Visualize the calculation process - Plot 2: Feature importance comparison
fig, ax = plt.subplots(1, 1, figsize=(7, 4))
features = ['Time', 'StressSurvey']
x = np.arange(len(features))
width = 0.35

bars1 = ax.bar(x - width/2, manual_importance, width, label='Manual Calculation', 
                color='lightblue', alpha=0.8, edgecolor='black')
bars2 = ax.bar(x + width/2, sklearn_importance, width, label='Sklearn Calculation', 
                color='lightcoral', alpha=0.8, edgecolor='black')

ax.set_xlabel('Features', fontsize=12)
ax.set_ylabel('Feature Importance', fontsize=12)
ax.set_title('Feature Importance: Manual vs Sklearn', fontsize=14, weight='bold')
ax.set_xticks(x)
ax.set_xticklabels(features)
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3, axis='y')

# Add value labels
for bars in [bars1, bars2]:
    for bar in bars:
        height = bar.get_height()
        ax.annotate(f'{height:.3f}', xy=(bar.get_x() + bar.get_width()/2, height),
                    xytext=(0, 3), textcoords="offset points", ha='center', va='bottom',
                    fontsize=10, weight='bold')

plt.tight_layout()
plt.show()

print("Key Insights:")
print("=" * 20)
print("• Feature importance measures how much each feature reduces prediction error")
print("• It's calculated by summing weighted impurity reductions across all splits")
print("• Features used in early splits (near root) tend to have higher importance")
print("• The values are normalized so they sum to 1.0")
print("• This gives us a measure of relative feature importance for anxiety prediction")
```

### The Critical Insight: Feature Importance Reveals the Truth

```{python}
#| label: fig-feature-importance-comparison
#| fig-cap: "Feature importance comparison: Decision trees vs Linear regression coefficients"
#| echo: false

# Get decision tree feature importance
tree_importance = tree_model.feature_importances_
feature_names = ['Time', 'StressSurvey']

# Create comparison plot - Plot 1: Decision tree feature importance
fig, ax = plt.subplots(1, 1, figsize=(7, 4))
bars1 = ax.bar(feature_names, tree_importance, color='steelblue', alpha=0.8, edgecolor='black')
ax.set_ylabel('Feature Importance', fontsize=12)
ax.set_title('Decision Tree Feature Importance', fontsize=14, weight='bold')
ax.grid(True, alpha=0.3, axis='y')

# Add value labels
for bar in bars1:
    height = bar.get_height()
    ax.annotate(f'{height:.3f}', xy=(bar.get_x() + bar.get_width()/2, height),
                xytext=(0, 3), textcoords="offset points", ha='center', va='bottom',
                fontsize=12, weight='bold')

plt.tight_layout()
plt.show()

# Create comparison plot - Plot 2: Linear regression coefficients (absolute values for comparison)
fig, ax = plt.subplots(1, 1, figsize=(7, 4))
lr_coef_abs = [abs(time_coef), abs(stress_coef)]
bars2 = ax.bar(feature_names, lr_coef_abs, color='lightcoral', alpha=0.8, edgecolor='black')
ax.set_ylabel('Absolute Coefficient Value', fontsize=12)
ax.set_title('Linear Regression Coefficients (Absolute)', fontsize=14, weight='bold')
ax.grid(True, alpha=0.3, axis='y')

# Add value labels
for bar in bars2:
    height = bar.get_height()
    ax.annotate(f'{height:.3f}', xy=(bar.get_x() + bar.get_width()/2, height),
                xytext=(0, 3), textcoords="offset points", ha='center', va='bottom',
                fontsize=12, weight='bold')

plt.tight_layout()
plt.show()

print("Feature Importance for Anxiety Prediction:")
print("=" * 50)
print(f"Decision Tree Feature Importance:")
print(f"  Time importance: {tree_importance[0]:.3f}")
print(f"  StressSurvey importance: {tree_importance[1]:.3f}")
print(f"\nLinear Regression Coefficients:")
print(f"  Time coefficient: {time_coef:.3f} (WRONG SIGN!)")
print(f"  StressSurvey coefficient: {stress_coef:.3f}")
print(f"\n🎯 For anxiety prediction:")
print(f"   • Decision trees correctly identify that BOTH features matter")
print(f"   • Linear regression gives Time the wrong sign due to")
print(f"     the non-linear StressSurvey relationship!")
print(f"   • This means our anxiety predictions from linear regression")
print(f"     are based on incorrect assumptions about how Time affects anxiety")
```

## Decision Tree Interpretation

### Reading Tree Rules for Anxiety Prediction

Decision trees provide interpretable rules that are easy to understand for anxiety prediction:

**Example Interpretation:**
- If StressSurvey ≤ 6 → Anxiety = 0.7 (low anxiety)
- If StressSurvey > 6 → Anxiety = 9.4 (high anxiety)

These rules tell us exactly how to predict anxiety based on the input variables, making the model transparent and actionable.

### Using the Tree for Social Media Recommendations

Now let's analyze what this tree tells us about social media time and anxiety:

```{python}
#| label: fig-tree-recommendations
#| fig-cap: "Analyzing the decision tree for social media recommendations"
#| echo: false

# Analyze the tree structure to understand Time's role
print("Decision Tree Analysis for Social Media Recommendations:")
print("=" * 60)

# Get the tree structure
tree = tree_model.tree_
feature_names = ['Time', 'StressSurvey']

print("Tree Structure Analysis:")
print(f"Root split: {feature_names[tree.feature[0]]} <= {tree.threshold[0]:.1f}")
print(f"Left child: {feature_names[tree.feature[tree.children_left[0]]]} <= {tree.threshold[tree.children_left[0]]:.1f}")
print(f"Right child: {feature_names[tree.feature[tree.children_right[0]]]} <= {tree.threshold[tree.children_right[0]]:.1f}")

print(f"\nKey Finding: The tree splits on StressSurvey first, then Time!")
print(f"This means StressSurvey is more important for anxiety prediction than Time.")

# Analyze what happens when we vary Time while holding StressSurvey constant
print(f"\nSocial Media Time Analysis:")
print(f"• When StressSurvey ≤ 6: Time doesn't matter much (anxiety stays low)")
print(f"• When StressSurvey > 6: Time still doesn't matter much (anxiety stays high)")
print(f"• The tree suggests StressSurvey is the primary driver of anxiety")

print(f"\nRecommendation:")
print(f"• Focus on stress management rather than just limiting social media time")
print(f"• Social media time alone may not be the main anxiety driver")
print(f"• Stress levels (measured by survey) are more predictive of anxiety")
```

::: {.callout-important}
## Interpreting the Tree for Social Media Policy

**What the tree tells us about social media and anxiety:**

1. **StressSurvey is the primary predictor**: The tree splits on StressSurvey first, not Time
2. **Time has limited impact**: Within each stress level, Time doesn't create additional splits
3. **Stress management is key**: The tree suggests addressing stress levels is more important than limiting social media time

**Policy implications:**
- **Don't just limit social media time**: The tree shows Time isn't the main anxiety driver
- **Focus on stress reduction**: StressSurvey is the primary predictor of anxiety
- **Holistic approach needed**: Anxiety appears to be driven more by underlying stress than social media usage

**Limitation**: This is a simplified tree with only 2 levels. In reality, Time might have more complex interactions that deeper trees could capture.
:::

### What Happens with More Depth? Time's Role in Deeper Trees

Let's see if Time becomes more important when we allow the tree to grow deeper:

```{python}
#| label: fig-deeper-tree
#| fig-cap: "Decision tree with depth=3 to explore Time's importance"
#| echo: false

# Create a deeper tree to see if Time becomes important
tree_deeper = DecisionTreeRegressor(max_depth=3, random_state=42)
tree_deeper.fit(data[['Time', 'StressSurvey']], data['Anxiety'])

# Visualize the deeper tree
fig, ax = plt.subplots(1, 1, figsize=(7, 4))
plot_tree(tree_deeper, feature_names=['Time', 'StressSurvey'], 
          filled=True, rounded=True, fontsize=8, ax=ax)
ax.set_title('Decision Tree with Depth=3: Does Time Matter More?', fontsize=14, weight='bold')
plt.tight_layout()
plt.show()

# Analyze the deeper tree structure
tree = tree_deeper.tree_
print("Deeper Tree Analysis (Depth=3):")
print("=" * 40)
print(f"Root split: {feature_names[tree.feature[0]]} <= {tree.threshold[0]:.1f}")

# Check if Time appears in the tree
time_splits = []
stress_splits = []
for i in range(tree.node_count):
    if tree.feature[i] != -2:  # Not a leaf node
        if tree.feature[i] == 0:  # Time feature
            time_splits.append(f"Node {i}: Time <= {tree.threshold[i]:.1f}")
        else:  # StressSurvey feature
            stress_splits.append(f"Node {i}: StressSurvey <= {tree.threshold[i]:.1f}")

print(f"\nTime splits: {len(time_splits)}")
print(f"StressSurvey splits: {len(stress_splits)}")
print(f"\nTime splits found: {time_splits}")
print(f"StressSurvey splits found: {stress_splits}")

# Calculate feature importance for deeper tree
deeper_importance = tree_deeper.feature_importances_
print(f"\nFeature Importance (Depth=3):")
print(f"Time: {deeper_importance[0]:.3f}")
print(f"StressSurvey: {deeper_importance[1]:.3f}")

print(f"\nComparison with Depth=2:")
print(f"Time importance increased: {deeper_importance[0] - tree_importance[0]:.3f}")
print(f"StressSurvey importance decreased: {tree_importance[1] - deeper_importance[1]:.3f}")
```

::: {.callout-tip}
## Key Insight: Time Becomes Important with More Depth

**What we discover with depth=3:**

1. **Time finally appears**: The deeper tree uses Time for splits, showing it does matter
2. **More balanced importance**: Time gets more weight in the deeper tree
3. **Complex interactions**: The tree can now capture how Time and StressSurvey interact

**Implication for social media policy:**
- **Deeper analysis reveals Time matters**: With more complexity, social media time does affect anxiety
- **Context-dependent effects**: Time's impact depends on stress levels
- **Policy nuance needed**: Simple "limit social media" may be too simplistic - the relationship is more complex

**Trade-off**: Deeper trees are more accurate but less interpretable. The depth=2 tree gives simple rules, while depth=3 reveals more nuanced relationships.
:::


## Strengths and Limitations

### Strengths of Decision Trees

::: {.callout-tip}
## Key Advantages

- **Interpretability**: Easy to understand and explain
- **No Assumptions**: Don't require linear relationships
- **Feature Interactions**: Naturally capture interactions between variables
- **Robust to Outliers**: Less sensitive to extreme values
- **Mixed Data Types**: Handle both numerical and categorical features
:::

### Limitations of Decision Trees

::: {.callout-warning}
## Key Disadvantages

- **Overfitting**: Can create overly complex trees that don't generalize
- **Instability**: Small data changes can create completely different trees
- **Poor Extrapolation**: Don't predict well outside training data range
- **Step Functions**: Create discontinuous predictions (not smooth)
- **Bias**: Tend to favor features with many possible splits
:::

### The Smoothness Problem

```{python}
#| label: fig-smoothness-problem
#| fig-cap: "Decision trees create step functions, not smooth curves"
#| echo: false

# Create a more detailed example using StressSurvey (since our tree uses 2 features)
detailed_stress = np.linspace(0, 12, 1000)
# For the tree, we need to provide both features, so we'll use mean Time value
mean_time = data['Time'].mean()
tree_input = np.column_stack([np.full(1000, mean_time), detailed_stress])
tree_detailed = tree_model.predict(tree_input)

# For linear regression, we need to use the same approach
lr_input = np.column_stack([np.full(1000, mean_time), detailed_stress])
lr_detailed = lr_multi.predict(lr_input)

fig, ax = plt.subplots(figsize=(7, 4))

# Plot both predictions
ax.plot(detailed_stress, lr_detailed, 'b-', linewidth=3, 
        label='Multiple Regression (Smooth)', alpha=0.8)
ax.plot(detailed_stress, tree_detailed, 'r-', linewidth=3, 
        label='Decision Tree (Step Function)', alpha=0.8)

# Extract actual split points from the tree model
def get_split_points(tree_model, feature_names):
    """Extract all split points for a given feature from the tree"""
    tree = tree_model.tree_
    split_points = []
    
    for i in range(tree.node_count):
        if tree.feature[i] != -2:  # Not a leaf node
            feature_idx = tree.feature[i]
            if feature_names[feature_idx] == 'StressSurvey':  # Only StressSurvey splits
                split_points.append(tree.threshold[i])
    
    return sorted(split_points)

# Get actual split points for StressSurvey
stress_split_points = get_split_points(tree_model, ['Time', 'StressSurvey'])

# Highlight the actual discontinuity points
for i, split_point in enumerate(stress_split_points):
    ax.axvline(x=split_point, color='red', linestyle='--', alpha=0.7, linewidth=2)
    ax.text(split_point, 8, f'Split {i+1}\n{split_point:.1f}', 
            rotation=90, ha='right', va='top', color='red', fontsize=10, weight='bold')

ax.set_xlabel('Stress Survey Score', fontsize=12)
ax.set_ylabel('Predicted Anxiety Level', fontsize=12)
ax.set_title('Smoothness Comparison: Linear vs Tree Models', fontsize=14, weight='bold')
ax.legend(fontsize=12)
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("The Smoothness Problem:")
print("=" * 30)
print("• Multiple regression: Smooth, continuous predictions")
print("• Decision trees: Step functions with sudden jumps")
print("• Real-world implication: Small changes in input can cause large prediction changes")
print("• Note: This shows predictions as StressSurvey varies (holding Time constant)")
```

## When to Use Decision Trees

### Ideal Scenarios

- **Non-linear relationships**: When linear models fail to capture the true relationship
- **Feature interactions**: When variables interact in complex ways
- **Interpretability requirements**: When stakeholders need to understand the model
- **Mixed data types**: When you have both numerical and categorical features
- **Robustness to outliers**: When your data contains extreme values

### When to Avoid

- **Linear relationships**: When the true relationship is approximately linear
- **Smooth predictions needed**: When you need continuous, smooth outputs
- **Small datasets**: When you don't have enough data to build reliable splits
- **High-dimensional data**: When you have many features relative to observations

## Advanced Decision Tree Concepts

### Tree Pruning

To prevent overfitting, trees can be pruned by removing branches that don't significantly improve performance:

```{python}
#| label: fig-tree-pruning
#| fig-cap: "Effect of tree depth on model complexity"
#| echo: false

# Create trees with different depths
depths = [1, 2, 3, 4]

for i, depth in enumerate(depths):
    fig, ax = plt.subplots(1, 1, figsize=(7, 4))
    
    # Fit tree with specific depth using both features
    tree_dep = DecisionTreeRegressor(max_depth=depth, random_state=42)
    tree_dep.fit(data[['Time', 'StressSurvey']], data['Anxiety'])
    
    # Plot actual vs predicted
    train_pred = tree_dep.predict(data[['Time', 'StressSurvey']])
    ax.scatter(data['Anxiety'], train_pred, 
              color='steelblue', s=80, alpha=0.7, edgecolors='black', label='Data')
    ax.plot([0, 15], [0, 15], 'k--', linewidth=2, alpha=0.8, label='Perfect Prediction')
    
    # Calculate R²
    r2 = r2_score(data['Anxiety'], train_pred)
    
    ax.set_xlabel('Actual Anxiety', fontsize=10)
    ax.set_ylabel('Predicted Anxiety', fontsize=10)
    ax.set_title(f'Tree Depth = {depth} (R² = {r2:.3f})', fontsize=12, weight='bold')
    ax.legend(fontsize=9)
    ax.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()
```

### Ensemble Methods

Decision trees are often combined in ensembles (Random Forest, Gradient Boosting) to improve performance while maintaining interpretability.

## Conclusion: Choosing the Best Approach for Anxiety Prediction

Our anxiety prediction journey showed us:

### **Progressive Improvement in Prediction Accuracy:**
1. **0 Variables (Baseline)**: Predict everyone has mean anxiety
2. **1 Variable (Time)**: Significant improvement by capturing time-anxiety relationship
3. **2 Variables (Linear Regression)**: Further improvement but with misleading coefficients
4. **2 Variables (Decision Tree)**: Best accuracy with correct interpretations

### **Key Insights for Anxiety Prediction:**

**Decision trees offer advantages for anxiety prediction by:**
1. **Capturing non-linear relationships** between stress surveys and anxiety
2. **Providing interpretable rules** that clinicians can understand
3. **Handling complex interactions** between time and stress naturally
4. **Requiring minimal assumptions** about how anxiety develops

**However, they come with trade-offs:**
- **Step functions** instead of smooth anxiety curves
- **Potential overfitting** without proper regularization
- **Instability** to small data changes

### **Recommendation for Anxiety Prediction:**
Use decision trees when you need to capture non-linear patterns in anxiety development and value interpretability for clinical decision-making. Use linear models when relationships are approximately linear and you need smooth predictions for anxiety trajectories.

The choice depends on your specific anxiety prediction needs: linear models for smooth anxiety curves, decision trees for capturing complex, non-linear anxiety patterns with interpretable rules.

---

## Appendix: Toy Problem - Feature Importance Calculation

Let's work through a simple example to understand exactly how feature importance is calculated in decision trees.

### The Toy Dataset

Consider a simple dataset with 8 observations:

```{python}
#| label: fig-toy-dataset
#| fig-cap: "Toy dataset for feature importance calculation"
#| echo: false

# Create a simple toy dataset
toy_data = pd.DataFrame({
    'Feature_A': [1, 1, 2, 2, 3, 3, 4, 4],
    'Feature_B': [10, 20, 10, 20, 10, 20, 10, 20],
    'Target': [5, 15, 8, 18, 12, 22, 15, 25]
})

print("Toy Dataset:")
print("=" * 40)
print(toy_data)
print(f"\nDataset shape: {toy_data.shape}")
print(f"Target mean: {toy_data['Target'].mean():.2f}")
print(f"Target variance: {toy_data['Target'].var():.2f}")
```

### Building the Toy Decision Tree

Let's create a simple decision tree and see its structure:

```{python}
#| label: fig-toy-tree
#| fig-cap: "Toy decision tree structure"
#| echo: false

# Create a simple decision tree
toy_tree = DecisionTreeRegressor(max_depth=2, random_state=42)
toy_tree.fit(toy_data[['Feature_A', 'Feature_B']], toy_data['Target'])

# Visualize the tree
fig, ax = plt.subplots(1, 1, figsize=(10, 6))
plot_tree(toy_tree, feature_names=['Feature_A', 'Feature_B'], 
          filled=True, rounded=True, fontsize=12, ax=ax)
ax.set_title('Toy Decision Tree Structure', fontsize=14, weight='bold')
plt.tight_layout()
plt.show()

# Print tree information
tree = toy_tree.tree_
print("Tree Structure Details:")
print("=" * 30)
for i in range(tree.node_count):
    if tree.feature[i] != -2:  # Not a leaf node
        feature_name = ['Feature_A', 'Feature_B'][tree.feature[i]]
        print(f"Node {i}: Split on {feature_name} <= {tree.threshold[i]:.1f}")
        print(f"  Samples: {tree.n_node_samples[i]}")
        print(f"  Left child: {tree.children_left[i]}")
        print(f"  Right child: {tree.children_right[i]}")
        print()
    else:  # Leaf node
        print(f"Node {i}: Leaf with value {tree.value[i][0][0]:.2f}")
        print(f"  Samples: {tree.n_node_samples[i]}")
        print()
```

### Step-by-Step Feature Importance Calculation

Now let's manually calculate feature importance using the actual impurity reduction formula:

```{python}
#| label: fig-toy-importance-calculation
#| fig-cap: "Step-by-step feature importance calculation for toy tree"
#| echo: false

def calculate_impurity_reduction(node_idx, tree, X, y):
    """Calculate the actual impurity reduction for a specific node"""
    if tree.feature[node_idx] == -2:  # Leaf node
        return 0
    
    # Get the split
    feature_idx = tree.feature[node_idx]
    threshold = tree.threshold[node_idx]
    left_child = tree.children_left[node_idx]
    right_child = tree.children_right[node_idx]
    
    # Get samples at this node
    node_samples = tree.n_node_samples[node_idx]
    left_samples = tree.n_node_samples[left_child]
    right_samples = tree.n_node_samples[right_child]
    
    # Calculate weighted MSE reduction
    # MSE = sum((y - mean(y))^2) / n
    node_mse = tree.impurity[node_idx] * node_samples
    left_mse = tree.impurity[left_child] * left_samples
    right_mse = tree.impurity[right_child] * right_samples
    
    mse_reduction = node_mse - (left_mse + right_mse)
    
    return mse_reduction

# Calculate feature importance manually
feature_importance_manual = np.zeros(2)
feature_names = ['Feature_A', 'Feature_B']

print("Manual Feature Importance Calculation:")
print("=" * 50)

total_reduction = 0
for i in range(tree.node_count):
    if tree.feature[i] != -2:  # Not a leaf node
        feature_idx = tree.feature[i]
        feature_name = feature_names[feature_idx]
        
        # Calculate impurity reduction
        reduction = calculate_impurity_reduction(i, tree, toy_data[['Feature_A', 'Feature_B']], toy_data['Target'])
        
        # Add to feature importance
        feature_importance_manual[feature_idx] += reduction
        total_reduction += reduction
        
        print(f"Node {i}: Split on {feature_name} <= {tree.threshold[i]:.1f}")
        print(f"  Impurity reduction: {reduction:.4f}")
        print(f"  Running total for {feature_name}: {feature_importance_manual[feature_idx]:.4f}")
        print()

# Normalize
feature_importance_manual = feature_importance_manual / total_reduction

print("Final Results:")
print("=" * 20)
print(f"Total impurity reduction: {total_reduction:.4f}")
print(f"Feature_A importance: {feature_importance_manual[0]:.4f}")
print(f"Feature_B importance: {feature_importance_manual[1]:.4f}")
print(f"Sum: {np.sum(feature_importance_manual):.4f}")

# Compare with sklearn
sklearn_importance = toy_tree.feature_importances_
print(f"\nComparison with Sklearn:")
print(f"Feature_A: Manual={feature_importance_manual[0]:.4f}, Sklearn={sklearn_importance[0]:.4f}")
print(f"Feature_B: Manual={feature_importance_manual[1]:.4f}, Sklearn={sklearn_importance[1]:.4f}")
```

### Understanding the Calculation

```{python}
#| label: fig-toy-importance-breakdown
#| fig-cap: "Detailed breakdown of feature importance calculation"
#| echo: false

# Create a detailed breakdown visualization
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(14, 10))

# Plot 1: Original data
ax1.scatter(toy_data['Feature_A'], toy_data['Target'], 
           color='steelblue', s=100, alpha=0.7, edgecolors='black')
ax1.set_xlabel('Feature_A')
ax1.set_ylabel('Target')
ax1.set_title('Original Data: Feature_A vs Target')
ax1.grid(True, alpha=0.3)

# Plot 2: Data split by Feature_A
ax2.scatter(toy_data['Feature_A'], toy_data['Target'], 
           color='steelblue', s=100, alpha=0.7, edgecolors='black')
ax2.axvline(x=2.5, color='red', linestyle='--', linewidth=2, label='Split at 2.5')
ax2.set_xlabel('Feature_A')
ax2.set_ylabel('Target')
ax2.set_title('Split on Feature_A <= 2.5')
ax2.legend()
ax2.grid(True, alpha=0.3)

# Plot 3: Data split by Feature_B
ax3.scatter(toy_data['Feature_B'], toy_data['Target'], 
           color='steelblue', s=100, alpha=0.7, edgecolors='black')
ax3.axvline(x=15, color='green', linestyle='--', linewidth=2, label='Split at 15')
ax3.set_xlabel('Feature_B')
ax3.set_ylabel('Target')
ax3.set_title('Split on Feature_B <= 15')
ax3.legend()
ax3.grid(True, alpha=0.3)

# Plot 4: Feature importance comparison
features = ['Feature_A', 'Feature_B']
x = np.arange(len(features))
width = 0.35

bars1 = ax4.bar(x - width/2, feature_importance_manual, width, label='Manual Calculation', 
                color='lightblue', alpha=0.8, edgecolor='black')
bars2 = ax4.bar(x + width/2, sklearn_importance, width, label='Sklearn Calculation', 
                color='lightcoral', alpha=0.8, edgecolor='black')

ax4.set_xlabel('Features')
ax4.set_ylabel('Feature Importance')
ax4.set_title('Feature Importance Comparison')
ax4.set_xticks(x)
ax4.set_xticklabels(features)
ax4.legend()
ax4.grid(True, alpha=0.3, axis='y')

# Add value labels
for bars in [bars1, bars2]:
    for bar in bars:
        height = bar.get_height()
        ax4.annotate(f'{height:.3f}', xy=(bar.get_x() + bar.get_width()/2, height),
                    xytext=(0, 3), textcoords="offset points", ha='center', va='bottom',
                    fontsize=10, weight='bold')

plt.tight_layout()
plt.show()

print("Key Insights from Toy Example:")
print("=" * 35)
print("1. Feature importance is calculated by summing impurity reductions")
print("2. Each split contributes to the feature's total importance")
print("3. The reduction is weighted by the number of samples at each node")
print("4. Values are normalized so they sum to 1.0")
print("5. Features used in early splits tend to have higher importance")
print("6. The calculation matches sklearn's implementation exactly")
```

### Summary: What We Learned

::: {.callout-tip}
## Toy Problem Key Takeaways

**Feature importance calculation process:**

1. **For each split**: Calculate how much impurity (MSE) is reduced
2. **Weight by samples**: Multiply by the number of samples that go through that split
3. **Sum by feature**: Add up all the weighted reductions for each feature
4. **Normalize**: Divide by the total reduction to get proportions that sum to 1

**Why this matters:**
- **Transparency**: We can see exactly how the "black box" calculation works
- **Interpretability**: We understand why certain features are more important
- **Validation**: We can verify that our manual calculation matches the library
- **Intuition**: We see that features used in early splits get higher importance

**The toy example shows:**
- Feature_A gets higher importance because it's used in the root split
- Feature_B gets lower importance because it's used in a later split
- The calculation is mathematically precise and reproducible
:::

---

## Example Visuals for Presentation

```{python}
#| label: fig-presentation-baseline
#| fig-cap: "Predicting Anxiety Based on 15 Observations"
#| echo: false

# Create traditional dotplot with density function (all positive values)
fig, ax = plt.subplots(1, 1, figsize=(7.5, 5.5))

# Create dotplot with jitter above x-axis
y_jitter = np.random.normal(0.05, 0.01, len(data['Anxiety']))  # Small positive jitter
ax.scatter(data['Anxiety'], y_jitter, alpha=0.6, s=100, color='steelblue', 
           edgecolors='black', linewidth=1, label='Observed Anxiety Values')

# Add density curve (all positive)
anxiety_range = np.linspace(data['Anxiety'].min() - 1, data['Anxiety'].max() + 1, 100)
density = stats.gaussian_kde(data['Anxiety'])
density_values = density(anxiety_range)
# Scale density to fit nicely on the plot (all positive)
density_scaled = density_values / density_values.max() * 0.15
ax.fill_between(anxiety_range, 0, density_scaled, alpha=0.3, color='lightblue', label='Probability Density')

ax.set_xlabel('Anxiety Level', fontsize=12)
ax.set_ylabel('Density', fontsize=12)
ax.set_title('Predicting Anxiety Based on 15 Observations', fontsize=14, weight='bold')
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3)
ax.set_ylim(0, 0.2)  # All positive values

plt.tight_layout()
plt.show()
```

---

*"The best model is not always the most complex one, but the one that best serves your analytical purpose."*
